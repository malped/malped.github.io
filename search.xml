<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[[转]梯度下降优化算法综述]]></title>
    <url>%2F2018%2F03%2F23%2F%E8%BD%AC-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95%E7%BB%BC%E8%BF%B0%2F</url>
    <content type="text"><![CDATA[本文转载自：https://blog.csdn.net/google19890102/article/details/69942970 本文翻译自Sebastian Ruder的“An overview of gradient descent optimization algoritms”，作者首先在其博客中发表了这篇文章，其博客地址为：An overview of gradient descent optimization algoritms，之后，作者将其整理完放在了arxiv中，其地址为：An overview of gradient descent optimization algoritms，在翻译的过程中以作者发布在Arxiv的论文为主，参考其在博客中的内容。 摘要虽然梯度下降优化算法越来越受欢迎，但通常作为黑盒优化器使用，因此很难对其优点和缺点的进行实际的解释。本文旨在让读者对不同的算法有直观的认识，以帮助读者使用这些算法。在本综述中，我们介绍梯度下降的不同变形形式，总结这些算法面临的挑战，介绍最常用的优化算法，回顾并行和分布式架构，以及调研用于优化梯度下降的其他的策略。 1 引言梯度下降法是最著名的优化算法之一，也是迄今优化神经网络时最常用的方法。同时，在每一个最新的深度学习库中都包含了各种优化的梯度下降法的实现（例如：参见lasagne，caffe和keras的文档）。然而，这些算法通常是作为黑盒优化器使用，因此，很难对其优点和缺点的进行实际的解释。 本文旨在让读者对不同的优化梯度下降的算法有直观的认识，以帮助读者使用这些算法。在第2部分，我们首先介绍梯度下降的不同变形形式。在第3部分，我们将简要总结在训练的过程中所面临的挑战。随后，在第4部分，我们将介绍最常用的优化算法，包括这些算法在解决以上挑战时的动机以及如何得到更新规则的推导形式。在第5部分，我们将简单讨论在并行和分布式环境中优化梯度下降的算法和框架。最后，在第6部分，我们将思考对优化梯度下降有用的一些其他策略。 梯度下降法是最小化目标函数$J(θ)$的一种方法，其中，$θ∈ℝ^d$为模型参数，梯度下降法利用目标函数关于参数的梯度$∇_θJ(θ)$的反方向更新参数。学习率$η$决定达到最小值或者局部最小值过程中所采用的步长的大小。即，我们沿着目标函数的斜面下降的方向，直到到达谷底。如果你对梯度下降法不熟悉，你可以从http://cs231n.github.io/optimization-1/找到介绍神经网络优化的材料。 2 梯度下降法的变形形式梯度下降法有$3$种变形形式，它们之间的区别为我们在计算目标函数的梯度时使用到多少数据。根据数据量的不同，我们在参数更新的精度和更新过程中所需要的时间两个方面做出权衡。 2.1 批梯度下降法$Vanilla$梯度下降法，又称为批梯度下降法$（batch gradient descent）$，在整个训练数据集上计算损失函数关于参数θ的梯度： θ=θ−η⋅∇_θJ(θ)因为在执行每次更新时，我们需要在整个数据集上计算所有的梯度，所以批梯度下降法的速度会很慢，同时，批梯度下降法无法处理超出内存容量限制的数据集。批梯度下降法同样也不能在线更新模型，即在运行的过程中，不能增加新的样本。 批梯度下降法的代码如下所示： 123for i in range(nb_epochs): params_grad = evaluate_gradient(loss_function, data, params) params = params - learning_rate * params_grad 对于给定的迭代次数，首先，我们利用全部数据集计算损失函数关于参数向量params的梯度向量params_grad。注意，最新的深度学习库中提供了自动求导的功能，可以有效地计算关于参数梯度。如果你自己求梯度，那么，梯度检查是一个不错的主意（关于如何正确检查梯度的一些技巧可以参见http://cs231n.github.io/neural-networks-3/）。 然后，我们利用梯度的方向和学习率更新参数，学习率决定我们将以多大的步长更新参数。对于凸误差函数，批梯度下降法能够保证收敛到全局最小值，对于非凸函数，则收敛到一个局部最小值。 2.2 随机梯度下降法相反，随机梯度下降法$（stochastic gradient descent, SGD）$根据每一条训练样本$x^{(i)}$和标签$y^{(i)}$更新参数： θ=θ−η⋅∇_θJ(θ;x^{(i)};y^{(i)})对于大数据集，因为批梯度下降法在每一个参数更新之前，会对相似的样本计算梯度，所以在计算过程中会有冗余。而$SGD$在每一次更新中只执行一次，从而消除了冗余。因而，通常$SGD$的运行速度更快，同时，可以用于在线学习。$SGD$以高方差频繁地更新，导致目标函数出现如图$1$所示的剧烈波动。 与批梯度下降法的收敛会使得损失函数陷入局部最小相比，由于$SGD$的波动性，一方面，波动性使得$SGD$可以跳到新的和潜在更好的局部最优。另一方面，这使得最终收敛到特定最小值的过程变得复杂，因为SGD会一直持续波动。然而，已经证明当我们缓慢减小学习率，SGD与批梯度下降法具有相同的收敛行为，对于非凸优化和凸优化，可以分别收敛到局部最小值和全局最小值。与批梯度下降的代码相比，SGD的代码片段仅仅是在对训练样本的遍历和利用每一条样本计算梯度的过程中增加一层循环。注意，如$6.1$节中的解释，在每一次循环中，我们打乱训练样本。 12345for i in range(nb_epochs): np.random.shuffle(data) for example in data: params_grad = evaluate_gradient(loss_function, example, params) params = params - learning_rate * params_grad 2.3 小批量梯度下降法小批量梯度下降法最终结合了上述两种方法的优点，在每次更新时使用$n$个小批量训练样本： θ=θ−η⋅∇_θJ(θ;x^{(i:i+n)};y^{(i:i+n)})这种方法: 减少参数更新的方差，这样可以得到更加稳定的收敛结果； 可以利用最新的深度学习库中高度优化的矩阵优化方法，高效地求解每个小批量数据的梯度。通常，小批量数据的大小在50到256之间，也可以根据不同的应用有所变化。当训练神经网络模型时，小批量梯度下降法是典型的选择算法，当使用小批量梯度下降法时，也将其称为$SGD$。注意：在下文的改进的$SGD$中，为了简单，我们省略了参数$x^{(i:i+n)};y^{(i:i+n)}$。 在代码中，不是在所有样本上做迭代，我们现在只是在大小为50的小批量数据上做迭代： 12345for i in range(nb_epochs): np.random.shuffle(data) for batch in get_batches(data, batch_size=50): params_grad = evaluate_gradient(loss_function, batch, params) params = params - learning_rate * params_grad 3 挑战虽然$Vanilla$小批量梯度下降法并不能保证较好的收敛性，但是需要强调的是，这也给我们留下了如下的一些挑战： 选择一个合适的学习率可能是困难的。学习率太小会导致收敛的速度很慢，学习率太大会妨碍收敛，导致损失函数在最小值附近波动甚至偏离最小值。 学习率调整$[17]$试图在训练的过程中通过例如退火的方法调整学习率，即根据预定义的策略或者当相邻两代之间的下降值小于某个阈值时减小学习率。然而，策略和阈值需要预先设定好，因此无法适应数据集的特点$[4]$。 此外，对所有的参数更新使用同样的学习率。如果数据是稀疏的，同时，特征的频率差异很大时，我们也许不想以同样的学习率更新所有的参数，对于出现次数较少的特征，我们对其执行更大的学习率。 高度非凸误差函数普遍出现在神经网络中，在优化这类函数时，另一个关键的挑战是使函数避免陷入无数次优的局部最小值。$Dauphin$等人$[5]$指出出现这种困难实际上并不是来自局部最小值，而是来自鞍点，即那些在一个维度上是递增的，而在另一个维度上是递减的。这些鞍点通常被具有相同误差的点包围，因为在任意维度上的梯度都近似为0，所以$SGD$很难从这些鞍点中逃开。 4 梯度下降优化算法下面，我们将列举一些算法，这些算法被深度学习社区广泛用来处理前面提到的挑战。我们不会讨论在实际中不适合在高维数据集中计算的算法，例如诸如牛顿法的二阶方法。 4.1 动量法$SGD$很难通过陡谷，即在一个维度上的表面弯曲程度远大于其他维度的区域[19]，这种情况通常出现在局部最优点附近。在这种情况下，$SGD$摇摆地通过陡谷的斜坡，同时，沿着底部到局部最优点的路径上只是缓慢地前进，这个过程如图$2a$所示。 $图2$ 如图$2b$所示，动量法$[16]$是一种帮助$SGD$在相关方向上加速并抑制摇摆的一种方法。动量法将历史步长的更新向量的一个分量γ增加到当前的更新向量中（部分实现中交换了公式中的符号） vt=γv_{t−1}+η∇_θJ(θ)$θ=θ−v_t$ 从本质上说，动量法，就像我们从山上推下一个球，球在滚下来的过程中累积动量，变得越来越快，（类似惯性？）（直到达到终极速度，如果有空气阻力的存在，则γ&lt;1）。同样的事情也发生在参数的更新过程中：对于在梯度点处具有相同的方向的维度，其动量项增大，对于在梯度点处改变方向的维度，其动量项减小。因此，我们可以得到更快的收敛速度，同时可以减少摇摆。 4.2 $Nesterov$加速梯度下降法然而，球从山上滚下的时候，盲目地沿着斜率方向，往往并不能令人满意。我们希望有一个智能的球，这个球能够知道它将要去哪，以至于在重新遇到斜率上升时能够知道减速。 $Nesterov$加速梯度下降法（$Nesterov accelerated gradient，NAG$）$[13]$是一种能够给动量项这样的预知能力的方法。我们知道，我们利用动量项$γv_{t−1}$来更新参数θ。通过计算$θ−γv_{t−1}$能够告诉我们参数未来位置的一个近似值（梯度并不是完全更新），这也就是告诉我们参数大致将变为多少。通过计算关于参数未来的近似位置的梯度，而不是关于当前的参数θ的梯度，我们可以高效的求解 ： vt=γv_{t−1}+η∇_θJ(θ−γv_{t−1})θ=θ−v_t自己的理解：在动量的基础上求梯度求得是双倍移动后的梯度（其实并没有双倍移动，只是不是用当前点的梯度了，用的是上次移动后更前面的点，这样梯度可能更陡，这次下降将更快） 同时，我们设置动量项γγ大约为0.9。动量法首先计算当前的梯度值（图3中的小的蓝色向量），然后在更新的累积梯度（大的蓝色向量）方向上前进一大步，Nesterov加速梯度下降法NAG首先在先前累积梯度（棕色的向量）方向上前进一大步，计算梯度值，然后做一个修正（绿色的向量）。这个具有预见性的更新防止我们前进得太快，同时增强了算法的响应能力，这一点在很多的任务中对于RNN的性能提升有着重要的意义[2]。 对于NAG的直观理解的另一种解释可以参见http://cs231n.github.io/neural-networks-3/，同时Ilya Sutskever在其博士论文[18]中给出更详细的综述。 既然我们能够使得我们的更新适应误差函数的斜率以相应地加速SGD，我们同样也想要使得我们的更新能够适应每一个单独参数，以根据每个参数的重要性决定大的或者小的更新。 4.3 $Adagrad$$Adagrad[7]$是这样的一种基于梯度的优化算法：让学习率适应参数，对于出现次数较少的特征，我们对其采用更大的学习率，对于出现次数较多的特征，我们对其采用较小的学习率。因此，Adagrad非常适合处理稀疏数据。Dean等人[6]发现Adagrad能够极大提高了SGD的鲁棒性并将其应用于Google的大规模神经网络的训练，其中包含了YouTube视频中的猫的识别。此外，Pennington等人[15]利用Adagrad训练Glove词向量，因为低频词比高频词需要更大的步长。 前面，我们每次更新所有的参数θ时，每一个参数$θ_i$都使用的是相同的学习率η。由于$Adagrad$在$t$时刻对每一个参数$θ_i$使用了不同的学习率，我们首先介绍$Adagrad$对每一个参数的更新，然后我们对其向量化。为了简洁，令$g_{t,i}$为在$t$时刻目标函数关于参数$θ_i$的梯度： g_{t,i}=∇_θJ(θ_i)在$t$时刻，对每个参数$θ_i$的更新过程变为： θ_{t+1,i}=θ_{t,i}−η⋅g_{t,i}对于上述的更新规则，在$t$时刻，基于对$θ_i$计算过的历史梯度，$Adagrad$修正了对每一个参数$θ_i$的学习率： θ_{t+1,i}=θ_{t,i}−{\eta\over\sqrt{G_{t,ii}+\epsilon}}⋅g_{t,i}其中，$G_t∈ℝ^{d×d}$是一个对角矩阵，对角线上的元素$[i,i]$是直到$t$时刻为止，所有关于$θ_i$的梯度的平方和（$Duchi$等人$[7]$将该矩阵作为包含所有先前梯度的外积的完整矩阵的替代，因为即使是对于中等数量的参数$d$，矩阵的均方根的计算都是不切实际的。）,ϵ是平滑项，用于防止除数为0（通常大约设置为$1e−8$）。比较有意思的是，如果没有平方根的操作，算法的效果会变得很差。 由于$G_t$的对角线上包含了关于所有参数θ的历史梯度的平方和，现在，我们可以通过$G_t$和$g_t$之间的元素向量乘法⊙向量化上述的操作： \theta_{t+1}=\theta_t-{\eta\over\sqrt{\mathbf G_t+\epsilon}}⊙\mathbf g_t$Adagrad$算法的一个主要优点是无需手动调整学习率。在大多数的应用场景中，通常采用常数$0.01$。 $Adagrad$的一个主要缺点是它在分母中累加梯度的平方：由于没增加一个正项，在整个训练过程中，累加的和会持续增长。这会导致学习率变小以至于最终变得无限小，在学习率无限小时，$Adagrad$算法将无法取得额外的信息。接下来的算法旨在解决这个不足。 4.4 Adadelta$Adadelta[21]$是$Adagrad$的一种扩展算法，以处理$Adagrad$学习速率单调递减的问题。不是计算所有的梯度平方，$Adadelta$将计算计算历史梯度的窗口大小限制为一个固定值$w$。 在$Adadelta$中，无需存储先前的$w$个平方梯度，而是将梯度的平方递归地表示成所有历史梯度平方的均值。在$t$时刻的均值$E[g^2]_t$只取决于先前的均值和当前的梯度（分量γ类似于动量项）： E[g^2]_t=γE[g^2]_{t−1}+(1−γ)g^2_t我们将γ设置成与动量项相似的值，即$0.9$左右。为了简单起见，我们利用参数更新向量$Δθ_t$重新表示$SGD$的更新过程： Δθ_t=−η⋅g_{t,i}\theta_{t+1}=\theta_t+\Delta\theta_t我们先前得到的Adagrad参数更新向量变为： \theta_{t+1}=\theta_t-{\eta\over\sqrt{\mathbf G_t+\epsilon}}⊙\mathbf g_t现在，我们简单将对角矩阵GtGt替换成历史梯度的均值$E[g^2]_t$： \theta_{t+1}=\theta_t-{\eta\over\sqrt{E[g^2]_t+\epsilon}}g_t由于分母仅仅是梯度的均方根$（root mean squared，RMS）$误差，我们可以简写为： \theta_{t+1}=\theta_t-{\eta\over RMS[g]_t}g_t作者指出上述更新公式中的每个部分（与SGD，动量法或者Adagrad）并不一致，即更新规则中必须与参数具有相同的假设单位。为了实现这个要求，作者首次定义了另一个指数衰减均值，这次不是梯度平方，而是参数的平方的更新： E[Δθ^2]t=γE[Δθ^2]_{t−1}+(1−γ)Δθ^2t因此，参数更新的均方根误差为： RMS[\Delta\theta^2]_t=\sqrt{E[\Delta\theta^2]_t+\epsilon}由于$RMS[Δθ]_t$是未知的，我们利用参数的均方根误差来近似更新。利用$RMS[Δθ]_{t−1}$替换先前的更新规则中的学习率η，最终得到$Adadelta$的更新规则： \Delta\theta_t=-{RMS[\Delta\theta]_{t-1}\over RMS[g]_t}g_t\theta_{t+1}=\theta_t+\Delta\theta_t使用$Adadelta$算法，我们甚至都无需设置默认的学习率，因为更新规则中已经移除了学习率。 4.5 $RMSprop$$RMSprop$是一个未被发表的自适应学习率的算法，该算法由$Geoff Hinton$在其Coursera课堂的课程6e中提出。 $RMSprop$和$Adadelta$在相同的时间里被独立的提出，都起源于对Adagrad的极速递减的学习率问题的求解。实际上，$RMSprop$是先前我们得到的$Adadelta$的第一个更新向量的特例： E[g^2]_t=0.9E[g^2]_{t−1}+0.1g^2_t\theta_{t+1}=\theta_t-{\eta\over \sqrt{E[g^2]_t+\epsilon}}g_t同样，$RMSprop$将学习率分解成一个平方梯度的指数衰减的平均。$Hinton$建议将$γ$设置为$0.9$，对于学习率η，一个好的固定值为$0.001$。 4.6 Adam自适应矩估计$（Adaptive Moment Estimation，Adam）[9]$是另一种自适应学习率的算法，$Adam$对每一个参数都计算自适应的学习率。除了像$Adadelta$和$RMSprop$一样存储一个指数衰减的历史平方梯度的平均$v_t$，$Adam$同时还保存一个历史梯度的指数衰减均值$m_t$，类似于动量： m_t=β_1m_{t−1}+(1−β_1)g_tv_t=β_2v_{t−1}+(1−β_2)g^2_t$m_t$和$v_t$分别是对梯度的一阶矩（均值）和二阶矩（非确定的方差）的估计，正如该算法的名称。当$m_t$和$v_t$初始化为$0$向量时，$Adam$的作者发现它们都偏向于$0$，尤其是在初始化的步骤和当衰减率很小的时候（例如$β_1$和$β_2$趋向于1）。 通过计算偏差校正的一阶矩和二阶矩估计来抵消偏差： m̂ _t={m_t\over 1-\beta^t_1}\hat v_t={v_t\over 1-\beta^T_2}正如我们在$Adadelta$和$RMSprop$中看到的那样，他们利用上述的公式更新参数，由此生成了$Adam$的更新规则： \theta_{t+1}=\theta_t-{\eta\over\sqrt{\hat v_t}+\epsilon}\hat m_t作者建议$β_1$取默认值为$0.9$,$β2$为$0.999$，ϵ为$10^{−8}$。他们从经验上表明$Adam$在实际中表现很好，同时，与其他的自适应学习算法相比，其更有优势。 4.7 算法可视化 4.8 选择使用哪种优化算法？那么，我们应该选择使用哪种优化算法呢？如果输入数据是稀疏的，选择任一自适应学习率算法可能会得到最好的结果。选用这类算法的另一个好处是无需调整学习率，选用默认值就可能达到最好的结果。 总的来说，$RMSprop$是$Adagrad$的扩展形式，用于处理在$Adagrad$中急速递减的学习率。$RMSprop$与$Adadelta$相同，所不同的是$Adadelta$在更新规则中使用参数的均方根进行更新。最后，$Adam$是将偏差校正和动量加入到$RMSprop$中。在这样的情况下，$RMSprop、Adadelta$和$Adam$是很相似的算法并且在相似的环境中性能都不错。$Kingma$等人$[9]$指出在优化后期由于梯度变得越来越稀疏，偏差校正能够帮助Adam微弱地胜过$RMSprop$。综合看来，$Adam$可能是最佳的选择。 有趣的是，最近许多论文中采用不带动量的$SGD$和一种简单的学习率的退火策略。已表明，通常$SGD$能够找到最小值点，但是比其他优化的$SGD$花费更多的时间，与其他算法相比，$SGD$更加依赖鲁棒的初始化和退火策略，同时，$SGD$可能会陷入鞍点，而不是局部极小值点。因此，如果你关心的是快速收敛和训练一个深层的或者复杂的神经网络，你应该选择一个自适应学习率的方法。 5 并行和分布式$SGD$当存在大量的大规模数据和廉价的集群时，利用分布式$SGD$来加速是一个显然的选择。$SGD$本身有固有的顺序：一步一步，我们进一步进展到最小。$SGD$提供了良好的收敛性，但$SGD$的运行缓慢，特别是对于大型数据集。相反，$SGD$异步运行速度更快，但客户端之间非最理想的通信会导致差的收敛。此外，我们也可以在一台机器上并行$SGD$，这样就无需大的计算集群。以下是已经提出的优化的并行和分布式的$SGD$的算法和框架。 5.1 $Hogwild!$$Niu$等人[14]提出称为$Hogwild!$的更新机制，$Hogwild!$允许在多个$CPU$上并行执行$SGD$更新。在无需对参数加锁的情况下，处理器可以访问共享的内存。这种方法只适用于稀疏的输入数据，因为每一次更新只会修改一部分参数。在这种情况下，该更新策略几乎可以达到一个最优的收敛速率，因为$CPU$之间不可能重写有用的信息。 5.2 $Downpour SGD$$Downpour SGD$是$SGD$的一种异步的变形形式，在$Google，Dean$等人[6]在他们的$DistBelief$框架（$TensorFlow$的前身）中使用了该方法。$Downpour SGD$在训练集的子集上并行运行多个模型的副本。这些模型将各自的更新发送给一个参数服务器，参数服务器跨越了多台机器。每一台机器负责存储和更新模型的一部分参数。然而，因为副本之间是彼此不互相通信的，即通过共享权重或者更新，因此可能会导致参数发散而不利于收敛。 5.3 延迟容忍$SGD$通过容忍延迟算法的开发，$McMahan$和$Streeter[11]$将$AdaGraad$扩展成并行的模式，该方法不仅适应于历史梯度，同时适应于更新延迟。该方法已经在实践中被证实是有效的。 5.4 $TensorFlow$TensorFlow[1]是Google近期开源的框架，该框架用于实现和部署大规模机器学习模型。TensorFlow是基于DistBelief开发，同时TensorFlow已经在内部用来在大量移动设备和大规模分布式系统的执行计算。在2016年4月发布的分布式版本依赖于图计算，图计算即是对每一个设备将图划分成多个子图，同时，通过发送、接收节点对完成节点之间的通信。 5.5 弹性平均$SGD$$Zhang$等人[22]提出的弹性平均$SGD（Elastic Averaging SGD，EASGD）$连接了异步$SGD$的参数客户端和一个弹性力，即参数服务器存储的一个中心变量。$EASGD$使得局部变量能够从中心变量震荡得更远，这在理论上使得在参数空间中能够得到更多的探索。经验表明这种增强的探索能力通过发现新的局部最优点，能够提高整体的性能。 6 优化$SGD$的其他策略最后，我们介绍可以与前面提及到的任一算法配合使用的其他的一些策略，以进一步提高$SGD$的性能。对于其他的一些常用技巧的概述可以参见[10]。 6.1 数据集的洗牌和课程学习总的来说，我们希望避免向我们的模型中以一定意义的顺序提供训练数据，因为这样会使得优化算法产生偏差。因此，在每一轮迭代后对训练数据洗牌是一个不错的主意。 另一方面，在很多情况下，我们是逐步解决问题的，而将训练集按照某个有意义的顺序排列会提高模型的性能和$SGD$的收敛性，如何将训练集建立一个有意义的排列被称为课程学习[3]。 $Zaremba and Sutskever[20]$只能使用课程学习训练LSTM来评估简单程序，并表明组合或混合策略比单一的策略更好，通过增加难度来排列示例。 6.2 批量归一化为了便于学习，我们通常用0均值和单位方差初始化我们的参数的初始值来归一化。 随着不断训练，参数得到不同的程度的更新，我们失去了这种归一化，随着网络变得越来越深，这种现象会降低训练速度，且放大参数变化。 批量归一化[8]在每次小批量数据反向传播之后重新对参数进行0均值单位方差标准化。通过将模型架构的一部分归一化，我们能够使用更高的学习率，更少关注初始化参数。批量归一化还充当正则化的作用，减少（有时甚至消除）$Dropout$的必要性。 6.3 $Early stopping$如$Geoff Hinton$所说：“$Early Stopping$是美丽好免费午餐”（NIPS 2015 Tutorial slides）。你因此必须在训练的过程中时常在验证集上监测误差，在验证集上如果损失函数不再显著地降低，那么应该提前结束训练。 6.4 梯度噪音$Neelakantan$等人[12]在每个梯度更新中增加满足高斯分布$N(0,σ^2_t)$的噪音： g_{t,i}=g_{t,i}+N(0,σ^2_t)高斯分布的方差需要根据如下的策略退火： σ^2_t={\eta\over (1+t)^{\gamma}}他们指出增加了噪音，使得网络对不好的初始化更加鲁棒，同时对深层的和复杂的网络的训练特别有益。他们猜测增加的噪音使得模型更优机会逃离当前的局部最优点，以发现新的局部最优点，这在更深层的模型中更加常见。 7 总结在这篇博客文章中，我们初步研究了梯度下降的三个变形形式，其中，小批量梯度下降是最受欢迎的。 然后我们研究了最常用于优化SGD的算法：动量法，$Nesterov$加速梯度，$Adagrad，Adadelta，RMSprop，Adam$以及不同的优化异步$SGD$的算法。 最后，我们已经考虑其他一些改善$SGD$的策略，如洗牌和课程学习，批量归一化和$early stopping。$ 参考文献 [1] Abadi, M., Agarwal, A., Barham, P., Brevdo, E., Chen, Z., Citro, C., … Zheng, X. (2015). TensorFlow : Large-Scale Machine Learning on Heterogeneous Distributed Systems. [2] Bengio, Y., Boulanger-Lewandowski, N., &amp; Pascanu, R. (2012). Advances in Optimizing Recurrent Networks. Retrieved from http://arxiv.org/abs/1212.0901 [3] Bengio, Y., Louradour, J., Collobert, R., &amp; Weston, J. (2009). Curriculum learning. Proceedings of the 26th Annual International Conference on Machine Learning, 41–48. http://doi.org/10.1145/1553374.1553380 [4] Darken, C., Chang, J., &amp; Moody, J. (1992). Learning rate schedules for faster stochastic gradient search. Neural Networks for Signal Processing II Proceedings of the 1992 IEEE Workshop, (September), 1–11. http://doi.org/10.1109/NNSP.1992.253713 [5] Dauphin, Y., Pascanu, R., Gulcehre, C., Cho, K., Ganguli, S., &amp; Bengio, Y. (2014). Identifying and attacking the saddle point problem in high-dimensional non-convex optimization. arXiv, 1–14. Retrieved from http://arxiv.org/abs/1406.2572 [6] Dean, J., Corrado, G. S., Monga, R., Chen, K., Devin, M., Le, Q. V, … Ng, A. Y. (2012). Large Scale Distributed Deep Networks. NIPS 2012: Neural Information Processing Systems, 1–11. http://doi.org/10.1109/ICDAR.2011.95 [7] Duchi, J., Hazan, E., &amp; Singer, Y. (2011). Adaptive Subgradient Methods for Online Learning and Stochastic Optimization. Journal of Machine Learning Research, 12, 2121–2159. Retrieved from http://jmlr.org/papers/v12/duchi11a.html [8] Ioffe, S., &amp; Szegedy, C. (2015). Batch Normalization : Accelerating Deep Network Training by Reducing Internal Covariate Shift. arXiv Preprint arXiv:1502.03167v3 [9] Kingma, D. P., &amp; Ba, J. L. (2015). Adam: a Method for Stochastic Optimization. International Conference on Learning Representations, 1–13. [10] LeCun, Y., Bottou, L., Orr, G. B., &amp; Müller, K. R. (1998). Efficient BackProp. Neural Networks: Tricks of the Trade, 1524, 9–50. http://doi.org/10.1007/3-540-49430-8_2 [11] Mcmahan, H. B., &amp; Streeter, M. (2014). Delay-Tolerant Algorithms for Asynchronous Distributed Online Learning. Advances in Neural Information Processing Systems (Proceedings of NIPS), 1–9. Retrieved from http://papers.nips.cc/paper/5242-delay-tolerant-algorithms-for-asynchronous-distributed-online-learning.pdf [12] Neelakantan, A., Vilnis, L., Le, Q. V., Sutskever, I., Kaiser, L., Kurach, K., &amp; Martens, J. (2015). Adding Gradient Noise Improves Learning for Very Deep Networks, 1–11. Retrieved from http://arxiv.org/abs/1511.06807 [13] Nesterov, Y. (1983). A method for unconstrained convex minimization problem with the rate of convergence o(1/k2). Doklady ANSSSR (translated as Soviet.Math.Docl.), vol. 269, pp. 543– 547. [14] Niu, F., Recht, B., Christopher, R., &amp; Wright, S. J. (2011). Hogwild! : A Lock-Free Approach to Parallelizing Stochastic Gradient Descent, 1–22. [15] Pennington, J., Socher, R., &amp; Manning, C. D. (2014). Glove: Global Vectors for Word Representation. Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, 1532–1543. http://doi.org/10.3115/v1/D14-1162 [16] Qian, N. (1999). On the momentum term in gradient descent learning algorithms. Neural Networks : The Official Journal of the International Neural Network Society, 12(1), 145–151. http://doi.org/10.1016/S0893-6080(98)00116-6 [17] H. Robinds and S. Monro, “A stochastic approximation method,” Annals of Mathematical Statistics, vol. 22, pp. 400–407, 1951. [18] Sutskever, I. (2013). Training Recurrent neural Networks. PhD Thesis. [19] Sutton, R. S. (1986). Two problems with backpropagation and other steepest-descent learning procedures for networks. Proc. 8th Annual Conf. Cognitive Science Society. [20] Zaremba, W., &amp; Sutskever, I. (2014). Learning to Execute, 1–25. Retrieved from http://arxiv.org/abs/1410.4615 [21] Zeiler, M. D. (2012). ADADELTA: An Adaptive Learning Rate Method. Retrieved from http://arxiv.org/abs/1212.5701 [22] Zhang, S., Choromanska, A., &amp; LeCun, Y. (2015). Deep learning with Elastic Averaging SGD. Neural Information Processing Systems Conference (NIPS 2015), 1–24. Retrieved from http://arxiv.org/abs/1412.6651]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>梯度下降</tag>
        <tag>转载</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[最优化问题及KKT条件的几何理解]]></title>
    <url>%2F2018%2F03%2F19%2F%E6%9C%80%E4%BC%98%E5%8C%96%E9%97%AE%E9%A2%98%E5%8F%8AKKT%E6%9D%A1%E4%BB%B6%E7%9A%84%E5%87%A0%E4%BD%95%E7%90%86%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[以二维空间$R^2$举例，从简单的无约束优化（0梯度条件），到等式约束优化（拉格朗日条件），再到不等式约束优化（KKT条件）解析优化问题解法的几何意义。 无约束优化问题$\min f(x)$，其中，$x=(x_1,x_2)$ 此时，对于$f(x)$的局部极小值点（红点）处梯度必然为0.因此优化问题可以转化为求解梯度为0的点。 带等式约束的优化问题$\min f(x)$，其中，$x=(x_1,x_2)$ $s.t. h(x)=0$ 与无约束问题不同，此时极小值点被限制在曲线$h(x)=0$上，我们因此将$\{x|h(x)=0\}$成为可行域，解只能在这个可行域里面取。如下图所示，曲线$h(x)=0$（黑色实线）便是可行域，现在要求在可行域上面的极小点。可以想象从无约束问题的极小点（黑点）靠等高线向外扩展，当扩展等高线第一次接触可行域时候（相切，梯度方向相反）的交点便是可行域的极小点。所以，相切，梯度方向相反，是取得极小值点的必要条件。 补充：能够碰到极大极小值点的必要条件是：梯度场与切空间垂直，也就是梯度场不能够有任何流形切空间上的分量，否则在切空间方向有分量，在流形上沿分量方向走，函数值会增加，沿反方向走，函数值会减少，不可能为局部极小或者极大值点。 两条曲线相切，梯度方向相反，即梯度差一个任意的常数乘子（取为$-\lambda$）：$\nabla f(x)=-\lambda \nabla h(x)$，调整后即可得到拉格朗日条件$\nabla (f(x)+\lambda h(x))=0$. 如此，带等式约束的优化问题转化为了无约束的优化问题，只需要对拉格朗日条件解方程组即可。这里$\lambda$便是拉格朗日乘子，有多少个等式约束就有多少个拉格朗日乘子。 带不等式约束的优化问题只有一个不等式起作用时$\min f(x)$ $s.t. h(x)\le0$ 当只有一个不等式起作用时，如下图所示，可行域变成了阴影部分，最小值点还是切点，跟等式约束条件完全一样，只需要把不等号当做等号去求解即可。 两个及以上不等式起作用时$\min f(x)$ $s.t.$ $g_1(x)\le0$ $g_2(x)\le0$ 如下图，当$f(x)$等高线慢慢扩大时，等高线与可行域（阴影部分）第一次相遇的点是个顶点，2个不等式同时起作用。满足最小值点从原来黑点的位置（切点）移动到了红点的位置，现在跟哪条约束函数都不相切了。这时候就需要用到KKt条件了。这里的条件是指：某一个点它如果是最小值点的话，就必须满足这个条件（在含不等式约束的优化问题里）。这是个必要条件，前面说的也全部是必要条件。 这个问题的解$x^*$应满足的KKT（卡罗需-库恩-塔克）条件为： $1. \mu_1\ge0, \mu_2\ge0;$ $2. \nabla f(x^￥)+\mu_1\nabla g_1(x^￥)+\mu_2\nabla g_2(x^￥)=0;$ $3. \mu_1g_1(x^￥)+\mu_2g_2(x^￥)=0$ 其中，$\mu$叫做KKT乘子，有多少个不等式约束就有多少个KKT乘子。加上本问题中的约束不封，就是完整版的KKT条件。对于有等式的情况：把其中一个不等式约束换成等式，可行域变成了半条曲线，最小值还是小红点，情况是一样的。 接下来看看KKT条件的几何意义。上图中绿色箭头为两条曲线的负梯度方向，红色箭头为等高线的梯度方向。如果这个顶点为满足约束的最小值点，那么该点处等高线梯度（红色箭头）一定在两个绿色箭头之间（$-\nabla g(x)$方向（绿色箭头）一定指向$g(x)$减小方向，即$g(x)\lt0$一边） $\mu_1\ge0,\mu_2\ge0$(红色箭头一定在绿色箭头之间)的解释：若三个向量的位置如下图所示，即$-\nabla f(x)$落在$\nabla g_1,\nabla g_2$所形成的锥形区域外侧。此时，作等高线（等值线）在点$x^k$处的切平面，可以发现：沿着与扶梯度$-\nabla f(x)$成锐角方向移动，只要能在红色阴影（阴影左界为当前等高线）取值，$f(x)$总能减小，而红色阴影区域为可行域，因此既可以建系哦啊目标函数值，又不破坏约束条件，所以当前$x^k$不是最优点。 有些不等式约束不起作用时如下面这个优化问题： $\min f(x)$ $s.t.$ $g_1(x)\le0$ $g_2(x)\le0$ $g_3(x)\le0$ 如下图$g_3(x)\le0$是不起作用的 对于最小值点$x^*$，三个不等式约束的不同在于： $g_1(x^￥)=0$（起作用） $g_2(x^￥)=0$（起作用） $g_3(x^￥)\lt0$（不起作用，最小值点不在$g_3(x)=0$上） 此时KKT条件1，2变为： $1. \mu_1\ge0, \mu_2\ge0, \mu_3\ge0, $ $2. \nabla f(x^￥)+\mu_1\nabla g_1(x^￥)+\mu_2\nabla g_2(x^￥)+\mu_3\nabla g_3(x^￥)=0$ 条件2中的$\mu_3\nabla g_3(x^￥)$让我们很苦恼，$g_3(x￥)$约束根本不起作用，要是能令$\mu_3=0$就好了。加上条件3： $3. \mu_1g_1(x^￥)+\mu_2g_2(x^￥)+\mu_3g_3(x^￥)=0$ 恰好能使$\mu_3=0​$。由于$g_1(x^￥)=0, g_2(x^￥)=0​$，所以前两项等于0，第三项$g_3(x^￥)\lt0​$,在条件3的作用下使得$\mu_3=0​$。正好满足哟啊求。如果再多几项不起作用的不等式约束，条件2都能在条件3的作用下实现：目标函数$f(x)​$的梯度$\nabla f(x)​$被起作用的不等式约束函数$g(x)​$的负梯度$-\nabla g(x)​$线性标出且系数$\mu​$全部非负（红色箭头被绿色箭头夹在中间）。这样，优化问题的求解就变成了对所有KKT条件解方程组。 如果再定义一个拉格朗日函数： $L(x,\mu)=f(x)+\mu_1g_1(x)+\mu_2g_2(x)+…$ 令它对$x$的偏导为0，就是KKT条件中的条件2了。 注意：以上所有都是局部极小值的点必要条件。据此求得的解不一定是局部极小值点（更别提全局），原因是上图中所画的等高线也许根本就不闭合，也急速实说我们一直想靠近的等高线和中间的黑点可能压根就是个鞍点或者近似鞍点。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>数学</tag>
        <tag>KKT</tag>
        <tag>最优化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GBDT&Xgboost]]></title>
    <url>%2F2018%2F03%2F17%2FGBDT%2F</url>
    <content type="text"><![CDATA[提升方法 在分类问题中，它通过改变训练样本的权重，学习多个分类器，并将这些分类器进行线性组合，他通过改变训练样本的权重，学习多个分类器，并将这些分类器进行线性组合，特高分类性能 对于提升方法，有两个问题需要回答：一是在每一轮如何改变训练数据的权值或概率分布；而是如何将弱分类器组合成一个强分类器。 ​ -《统计学习方法》 Adaboost $Adaboost$算法的特点是通过迭代每次学习一个基本分类器，每次迭代中，提高那些被前一轮分类器错误分类数据的权值，而降低那些被正确分类的数据的权值。最后$Adaboost$将基本分类器的线性组合作为强分类器，其中给分类误差率小的基本分类器大的权值，给分类误差率大的基本分类器小的权值 提升树提升树正是一种前向分步的加法模型，但是其基分类器是树模型(二叉树)，分为二叉分类树和二叉回归树，提升树往往在实践中表现非常好。提升树的模型如下: f_M(x)=\sum^M_{m=1}T(x;\Theta)其中$T(x;\Theta)$表示决策树，$M$为树的个数，$\Theta$表示树的参数。 首先确定初始提升树$f_0(x)=0$，第$m$步的模型是 f_m(x)=f_{m-1}(x)+T(x;\Theta_m)其中，$f_{m-1}(x)$为当前模型，通过经验风险极小化确定下一棵决策树的参数$\Theta_m$，第$m$步的模型是 \hat \Theta_m=\arg \min_{\Theta_m}\sum^N_{i=1}L(y_i,\ f_{m-1}(x_i)+T(x_i;\Theta_m))GBDT为什么拟合负梯度为什么要拟合负梯度（注意这里的梯度是对$\hat y_i^{t-1}$求导）呢？ 跟梯度下降类似，我们不断减去$\partial f(x)\over \partial x$可以得到$\min_x f(x)$，同理不断减去$\partial L\over\partial \hat y^{t-1}_i$就能得到$\min_{\hat y}L(\hat y)$。所以其实GBDT就是在函数空间的梯度下降。 传统GBDT算法算法流程： 乍一看，GBDT算法跟梯度下降算法很像，其实GBDT就是在函数空间的梯度下降，我们不断减去$\partial f(x)\over \partial x$可以得到$\min_x f(x)$，同理不断减去$\partial L\over\partial F_{K-1}$就能得到$\min_FL(F)$，注意这里是对$F_{K-1}$求导。 基于残差的GBDT推导$GBDT$全称$Gradient Boosting Decision Tree$, 其中$gradient$被称为梯度，更一般的理解，可以认为是一阶导，这里残差与梯度是什么关系呢。我们看平方损失函数${1\over 2}\sum^n_0(y_i-F(x_i))^2$，这个算是函数主要是针对回归类型的。 $ {1\over 2}\sum^n_{i=1}(y_i-F(x_i))^2$ $= {1\over 2}\sum^n_{i=1}(y_i-\hat y_i^t)^2$ $= {1\over 2}\sum^n_{i=1}(y_i-\hat y_i^{t-1}-f_t(x_i))^2$ 令其对$\hat y_i^{t-1}$求导为0 得： -\sum^n_{i=1}(y_i-\hat y_i^{t-1}-f_t(x_i))=0则： \sum_{i=1}^nf_t(x_i)=\sum_{i=1}^n(y_i-\hat y_i^{t-1})所以基于残差的gbdt是一种特殊的gbdt模型，它的损失函数是平方损失函数，只能处理回归类的问题。 为什么基于残差的gbdt不是一个好的选择首先基于残差的gbdt只能处理回归类的问题，不能处理分类问题，这是损失函数所限制的，所以更一般化的gbdt是基于梯度的算法，这也就意味着只要我给出的损失函数是可导的，那么我就能用gbdt的思想去解决问题。具体解决的问题就不会仅仅限于回归了。 另外，基于残差的gbdt在解决回归问题上也不算是一个好的选择，一个比较明显的缺点就是对异常值过于敏感。 Boosting的加法模型gbdt模型可以认为是是由k个基模型组成的一个加法运算式： 其中F是指所有基模型组成的函数空，以回归任务为例，回归树可以看作为一个把特征向量映射为某个score的函数。该模型的参数为：$\Theta=\{f_1,f_2,…,f_K\}$。于一般的机器学习算法不同的是，加法模型不是学习d维空间中的权重，而是直接学习函数（决策树）集合。 那么一般化的损失函数是预测值$\hat y$与 真实值$y$之间的关系，如我们前面的平方损失函数，那么对于n个样本来说，则可以写成： L=\sum^n_{i=1}l(y_i,\ \hat y_i)更一般的，我们知道一个好的模型，在偏差和方差上有一个较好的平衡，而算法的损失函数正是代表了模型的偏差面，最小化损失函数，就相当于最小化模型的偏差，但同时我们也需要兼顾模型的方差，所以目标函数还包括抑制模型复杂度的正则项，因此目标函数可以写成： Obj=\sum^n_{i=1}f_k(x_i)+\sum^K_{k=1}\Omega(f_k)其中$\Omega$代表了基模型的复杂度，若基模型是书模型，则树的深度、叶子节点数等指标可以反映树的复杂度。 对于Boosting来说，它采用的是前向分布算法，即从前往后，逐渐建立基模型来优化逼近目标函数，具体过程如下： 那么，在每一步，如何学习一个新的模型呢，答案的关键还是在于gbdt的目标函数上，即新模型的加入总是以优化目标函数为目的的。 我们一第$t$步的模型拟合为例，在这一步，模型对第$i$个样本$x_i$的预测为： \hat y_i^t=\hat y^{t-1}_i+f_t(x_i)其中$f_t(x_i)$就是我们这次需要加入的新模型，即需要拟合的模型，此时，目标函数就可以写成： 即此时最优化目标函数，就相当于求得了$f_t(x_i)$ $Xgboost$$Xgboost$的目标函数 泰勒公式：设$n$是一个正整数，如果定义在一个包含$a$的区间上的函数$f$在$a$点处$n+1$次可导，那么对于这个区间上的任意$x$都有：$f(x)=\sum^N_{n=0}{f^{(n)}(a)\over n!}(x-a)^n+R_n(x)$，其中的多项式称为函数在$a$处的泰勒展开式，$R_n(x)$是泰勒公式的余项且是$(x-a)^n$的高阶无穷小。 ​ ——维基百科 二阶泰勒展开： 那么在等式$(1)$中，我们把$\hat y^{t-1}_i$看成是等式$(2)$中的$x$，$f_t(x_i)$看成是$\Delta x$，因此等式$(1)$可以写成： 其中$g_i$和$h_i$分别为$l(y_i, \hat y_i^{t-1})$关于$\hat y_i^{t-1}$的一阶导和二阶导。我们一平方损失函数为例$\sum^n_{i=1}(y_i-(\hat y^{t-1}_i+f_t(x_i) ) )^2$，则$g_i=\partial _{\hat y^{t-1}}(\hat y^{t-1}-y_i)^2=2(\hat y^{t-1}-y_i)$，$h_i=\partial^2_{\hat y^{t-1}}(\hat y^{t-1}-y_i)^2=2$。 由于在第$t$步$\hat y^{t-1}_i$其实是一个已知的值，所以$l(y_i, \hat y^{t-1}_i)$是一个常数，其对函数优化不会产生影响，因此，等式$(3)$可以写成： 所以我们只需要求出前一步损失函数的一阶和二阶导的值（由于前一步的$\hat y^{t-1}$是已知的，所以这两个值就是常数）带入等式$(4)$，然后优化目标函数，就可得到每一步的$f(x)$，最后根据加法模型的到一个整体的模型。 如何用决策树来表示上一步的目标函数 假设我们$boosting$的基模型用决策树来实现，则一颗生成好的决策树，即结构确定，也就是说树的叶子结点其实是确定了的。假设这棵树的叶子节点有$T$片叶子，而每片叶子对应的值$w\in R^T$。熟悉决策树的同学应该清楚，每一片叶子结点中样本的预测值都会是一样的，在分类问题中，是某一类；在回归问题中，是某一个值，那么肯定存在这样一个函数$q:R^d\rightarrow \{1,2,…,T\}$，及将$f_t(x)$中的每个样本映射到每一个叶子节点上，当然$f_t(x)$和$q$我们都是不知道的，但我们也不关心，这里只是说明一下决策树表达数据结构的方法是怎么样的，不理解也没有问题。 那么$f_t(x)$就可以转成$w_{q(x)}$，这里的$q(x)$代表了每个样本在哪个叶子节点上，而$w_{q(x)}$则代表了哪个叶子结点去什么$w$值，所以$w_{q(x)}$就代表了每个样本的取值$w$（即预测值）。 如果决策树的复杂度可以由正则项来定义$\Omega(f_t)=\gamma T+{1\over2}\lambda\sum_{j=1}^Tw_j^2$ (这是xgb的正则项)，即决策树模型的复杂度由生成的树的叶子节点数量和叶子节点对应的值向量的$L2$范数决定。 我们假设$I_j=\{i|q(x_i)=j\}$为第$j$个叶子节点的样本集合，则等式$(4)$根据上面的一些变换可以写成： 即我们之前样本的集合，现在都改写成了叶子结点的集合，由于一个叶子结点有多个样本存在，因此才有了$\sum_{i\in I_j}g_i$和$\sum_{i\in I_j}h_i$这两项。 定义$G_j=\sum_{i\in I_j}g_i，H_j=\sum_{i\in I_j}h_i$，则等式$(5)$可以写成： Obj^{(t)}=\sum^T_{j=1}[G_jw_j+{1\over 2}(H_j+\lambda)w_j^2]+\gamma T如果树结构是固定的，即$q$是确定的，或者说我们已经知道了每个叶子结点有哪些样本，所以$G_j$和$H_j$是确定的，但$w$不确定（$w$其实就是我们要预测的值），那么令目标函数一阶导数为$0$，则可以求得叶子结点$j$对应的值： 目标函数的值可以化简为： 如何求树的结构（单线程版本） 那么对于单棵决策树，一种理想的优化状态就是枚举所有可能的树结构，因此过程如下： 首先枚举所有可能的树结构； 计算每种树结构下的目标函数值，即等式$(7)$的值； 取目标函数最小（大）值为最佳的数结构，根据等式6求得每个叶子节点的$w$取值，即样本的预测值。 然而，可能的树结构数量是无穷的，所以实际上我们不可能枚举所有可能的树结构。通常情况下，我们采用贪心策略来生成决策树的每个节点。 从深度为0的树开始，对每个叶节点枚举所有的可用特征 针对每个特征，把属于该节点的训练样本根据该特征值升序排列，通过线性扫描的方式来决定该特征的最佳分裂点，并记录该特征的最大收益（采用最佳分裂点时的收益） 选择收益最大的特征作为分裂特征，用该特征的最佳分裂点作为分裂位置，把该节点生长出左右两个新的叶节点，并为每个新节点关联对应的样本集 回到第1步，递归执行到满足特定条件为止 那么如何计算上面的收益呢，很简单，仍然紧扣目标函数就可以了。假设我们在某一节点上二分裂成两个节点，分别是左$（L）$右$（R）$，则分列前的目标函数是$-{1\over 2}[{(G_L+G_R)^2\over H_L+H_R+\lambda}] $，分裂后则是$-{1\over 2}[{G_L^2\over H_L+\lambda}+{G_R^2\over H_R+\lambda}]+2\lambda$，则对于目标函数来说，分裂后的收益是（这里假设是最小化目标函数，所以用分列前减去分裂后）： 等式$(8)$计算出来的收益，也是作为变量重要度输出的重要依据。 这时，就有两种后续。一种是当最好的分割的情况下，GainGain为负时就停止树的生长，这样的话效率会比较高也简单，但是这样就放弃了未来可能会有更好的情况。另外一种就是一直分割到最大深度，然后进行修剪，递归得把划分叶子得到的Gain为负的收回。一般来说，后一种要好一些，于是我们采用后一种，完整的算法如下（没有写修剪） 建树总结所以gbdt的算法可以总结为： 算法在拟合的每一步都新生成一颗决策树； 在拟合这棵树之前，需要计算损失函数在每个样本上的一阶导和二阶导，即$g_i$和$h_i$; 通过上面的贪心策略生成一棵树，计算每个叶子节点的$G_j$和$H_j$，利用等式$(6)$计算预测值$w$; 把新生成的决策树$f_t(x)$加入$\hat y^t_i=\hat y^{t-1}_i+\epsilon f_t(x_i)$，其中$\epsilon$为学习率，主要为了抑制模型的过拟合。 算法复杂度 按照某特征里的值进行排序，复杂度是$O(nlog n)$ 2. 扫描一遍该特征所有值得到最优分割点，因为该层（兄弟统一考虑）一共有$n$个样本，所以复杂度是$O(n)$ 3. 一共有$d$个特征，所以对于一层的操作，复杂度是$O(d(nlog n+n))=O(d nlog n)$ 4. 该树的深度为$k$。所以总复杂度是$O(k d nlog n)$ 近似算法近似算法思想枚举特征所有可能分割在计算上要求很高，为有效做到这一点，算法必须根据特征值对数据预先排序。贪心算法很强大，然而当数据量过大，无法装入整个内存中时，会非常不高效。 树模型对特征取值范围不敏感，只对顺序敏感。近似算法首先根据特征分布的百分位提出候选分裂点，然后将连续特征映射到这些候选分割点形成的bucket中，聚合统计数据，并基于聚合数据找到最佳解决方案。近似算法通常使用加权百分位树选择候选分割点，使特征按照权重期望均匀分布在候选分割点之间。 我们用$D_k=\{(x_{1k},h_1),(x_{2k},h_2),(x_{3k},h_3),…,(x_{nk},h_n)\}$代表每个样本的第$k$个特征和其对应的二阶梯度所组成的集合。那么我们就可以用百分比来定义这个排名函数$r_k:\mathbb{R}\rightarrow[0,1]$： r_k(z)={1\over \sum_{(x,h)\in D_k}h}\sum_{(x,h)\in D_k,\ x]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>GBDT</tag>
        <tag>xgboost</tag>
        <tag>梯度提升树</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[线性模型的判断]]></title>
    <url>%2F2018%2F03%2F10%2F%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%88%A4%E6%96%AD%2F</url>
    <content type="text"><![CDATA[线性模型的判断线性模型定义：如果模型是参数的线性函数，并且存在线性分类面就是线性模型，否则不是。 注意：线性模型可以用曲线拟合样本，但是分类的决策边界一定是直线的，例如$LR$ 例子： 单层感知机（网络）无法解决异或问题（单层感知机是线性模型） 存在非线性激活函数的多层感知机（网络）可以解决异或问题（存在非线性激活函数多层感知机是非线性） 快速判断线性非线性线性的情况 看乘法式子中自变量$x$前的系数$w$，如果$w$只影响一个$x$，那么此模型为线性模型。（等价于对决策边界是否为超平面的判断） 非线性的情况 $(变量)^n$，且$n$不为1为非线性 $|变量|$有变量在绝对值内为非线性 $sgn (变量)$有变量在符号函数内为非线性 常见的模型常见的线性分类器：$LR$，贝叶斯分类器，单层感知机，线性回归 常见的非线性分类器：决策树，$RF$，$GBDT$，多层感知机 $SVM$两种都有（看是线性核还是高斯核） 线性分类器速度快、编程方便，但是拟合效果可能不好 非线性分类器编程复杂，但是拟合能力强 为什么朴素贝叶斯分类器是线性分类器？这一部分我感觉我的理解出现了偏差，先写上了 朴素贝叶斯分类器假设了各特征之间相互独立，采用后验概率最大的类别 贝叶斯公式： p(C_k|x_1,x_2,…,x_n)={p(C_k)\Pi^n_{i=1}p(x_i|C_k)\over \sum_ip(x|C_i)p(C_i)}训练时候只需要统计$p(C_k),p(x_i|C_k)$，可以说是$very naive$了 从线性模型角度理解这里以二分类举例 我们将样本表示为$x=(x_1,x_2,…,x_n)$，$y=\{0,1\}$，$p_{ki}$表示$C_k$中$x_i$出现的概率 $\log p(C_k)p(x|C_k)$ $=\log p(C_k)+\sum_{i=1}^n\log p_{ki}$ 对于上式，将$\log p_{ki}$看做$w_i$，而$x_i$为样本中$x_i$表示的值的出现，$x_i=1$。上式可以化为： $=w^T_kx+b$]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>线性模型</tag>
        <tag>非线性模型</tag>
        <tag>朴素贝叶斯</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[说说生成模型与判别模型]]></title>
    <url>%2F2018%2F03%2F09%2F%E8%AF%B4%E8%AF%B4%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B%E4%B8%8E%E5%88%A4%E5%88%AB%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[本文主要是对知乎相关问题的总结 是什么？ 监督学习的任务就是学习一个模型，应用这一个模型，对给定的输入预测相应的输出。这个模型的一般形式为决策函数：$Y=f(X)$或者条件概率分布：$P(Y|X)$ 监督学习方法又可以氛围生成方法和判别方法。所学到的模型分别称为生成模型和判别模型。 生成模型书上的话：生成方法由数据学习联合概率分布P(X,Y)，然后求出条件概率分布$P(Y|X)$作为预测模型，即生成模型：$P(Y|X)={P(X,Y)\over P(X)}$ 这样的方法称为生成方法，是因为模型表示了$X,Y$的生成关系。典型的生成模型有：朴素贝叶斯法、隐马尔科夫模型、混合高斯模型、AODE、Latent Dirichlet allocation（unsup）、Restricted Boltzmann Machine 生成模型，就是生成（数据de分布）的模型，关注的是抓取样本的分布特征，通过建立每个类别的特征分布，最终建立很多类别模型（一般有多少类就有多少个），最后比较结果选择最优。生成模型输入$X$与输出$Y$之间没有因果关系。 判定模型书上的话：判别方法由数据直接学习决策函数$f(x)$或者条件概率分布$P(Y|X)$作为预测的模型，即判别模型。判别方法关心的是对给定的输入$X$应该预测什么样的输出$Y$。典型的判别方法包括：kNN，感知机，决策树，逻辑回归，最大熵模型，SVM，提升方法，条件随机场，神经网络等。 判别模型，就是判别（数据输出量）的模型，直接采用输入特征去预测输出，或者说条件概率。判定模型只有一个模型，$X$往里面丢，$label$就出来了。判别模型输入$X$与输出$Y$之间有因果关系 用起来？ 生成模型优点： 1.生成模型给出的是联合分布$P(\hat x,\hat c)$，不仅能够由联合分布计算条件分布$P(\hat c|\hat x)$（反之不行），还可以给出其他信息，比如可以使用$P(\hat x)=\sum^k_{i=1}P(\hat x|\hat c_i)P(\hat c_i)$来计算边缘分布$P(\hat x)$。如果一个输入样本的边缘分布$P(\hat x)$很小的话，那么可以认为学处的这个模型可能不太适合对这个样本进行分类，分类效果可能会不好，这技术里所谓的$outlier detection$。 2.生成模型收敛速度比较快，即当样本数量较多时，生成模型能更快地收敛于真实模型。3.生成模型能够应付存在隐变量的情况，比如混合高斯模型就是含有隐变量的生成方法。 4.生成模型所需计算资源是更少的，大部分情况下只要做统计计数就可以获得模型 缺点： 1.实践中多数情况下判别模型效果更好 判别模型优点： 1.直接面对预测，准确率往往较生成模型高 2.由于直接学习$P(\hat c|\hat x)$而不需要求解类别条件概率，所以允许我们对输入进行抽象（比如降维、构造等），从而能够简化学习问题。 缺点： 1.判别模型所需资源更多，因为判别模型必须靠误差梯度下降法来获得模型，训练所需时间要多很多。 过拟合： 生成模型‘‘没有考虑正则化很简单，因为他们很少过拟合’’。生成模型学习$X,Y$的联合概率分布$P(X,Y)$，直接学习的就是数据的分布，从整个数据的整体着手，很少会出现过拟合。 基本上属于高偏差/低方差分类器，当样本数量小于特征数量或样本数量不足时，应选用这种模型 判别模型判别模型应当有正则化过程，因为是直接生成$f(x)$或者$p(y|x)$，所以很容易比较$y$跟$f(x)$的关系，按照现有数据照葫芦画瓢来判别，容易过拟合，所以正则化便有存在的意义。 基本上属于低偏差/高方差分类器，容易过拟合，需要正则项。数据量充足时选用判别模型 结论随着训练集的增大，低偏差/高方差分类器（判别模型）相对于高偏差/低方差分类器（生成模型）准确率高，因为随着数据量的增大，现有训练集数据的分布更接近于真实分布，此时生成模型优势变小，同时生成模型不能提高足够的准确率，此时，判别模型优势更大。 举个例子： 当一份分类数据的特征维度大于样本数量时。如果采用判别模型，极端情况下每条样本都有唯一的特征（或特征组合），此时如果正则化不够给力，那么该判别模型将极大限度拟合当前数据，训练集AUC可能将近1，那么就可能得到训练数据上准确率100%，测试数据准确率不如XJB猜的模型。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>生成模型</tag>
        <tag>判别模型</tag>
        <tag>模型</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[拉格朗日对偶性]]></title>
    <url>%2F2018%2F03%2F03%2F%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E5%AF%B9%E5%81%B6%E6%80%A7%2F</url>
    <content type="text"><![CDATA[原始问题​ 假设$f(x), c_i(x), h_j(x)$是定义在$R^n$上面的连续可微函数，考虑约束最优化问题 \min_{x\in R^n}f(x)s.t.\ c_i(x)\le0,\ i=1,2,…,kh_j(x)=0,\ j=1,2,…,l ​​ 称此约束最优化问题为原始最优化问题或原始问题。 ​ 首先，引进广义拉格朗日函数 L(x,\alpha,\beta)=f(x)+\sum_{i=1}^k\alpha_ic_i(x)+\sum_{j=1}^l\beta_jh_j(x)​ 这里，$\alpha_i,\beta_j$是拉格朗日乘子，$\alpha_i\ge0$. ​ 考虑函数(注意：此时$L(x,\alpha,\beta)$的变量是$\alpha_i\beta_j$)： \Theta_P(x)=\max_{\alpha,\beta:\alpha_i\ge0}L(x,\alpha,\beta)​ 这里下标$P(Primal)$表示原始问题.对于此函数（关于$\alpha,\beta$的函数，$x$是常量），经过我们优化（不管什么方法），确定$\alpha,\beta$的值，就可以得到$L(x,\alpha,\beta)$的最大值，因为此时$\alpha,\beta$已经确定，显然最大值\Theta_P(x)=\max_{\alpha,\beta:\alpha_i\ge0}L(x,\alpha,\beta)就是只和$x$有关的函数 ​ 下面通过$x$是否绵竹约束条件两方面来分析这个函数： ​ 1.考虑某个$x$违反了原始的约束，即$c_i\gt0$或者$h_j\neq0$，那么： \Theta_P(x)=\max_{\alpha,\beta:\alpha_i\ge0}[f(x)+\sum_{i=1}^k\alpha_ic_i(x)+\sum_{j=1}^l\beta_jh_j(x)]=\infty​ 2.考虑$x$满足原始的约束，则： \Theta_P(x)=\max_{\alpha,\beta:\alpha_i\ge0}[f(x)]=f(x)​ 注意最大化确定$\alpha,\beta$的过程，$f(x)$就是个常量，常量的最大值显然是本身 ​ 通过上面两条分析可以得出： \Theta_P(x)=\begin{cases}f(x),\ {x满足原始问题约束}\\+\infty,\ {其他}\\\end{cases}​ 那么在满足约束的条件下： \min_x\Theta_P(x)=\min_x\max_{\alpha,\beta:\alpha_i\ge0}L(x,\alpha,\beta)=min_xf(x)​ 即$\min_x\Theta_P(x)$与原始优化问题等价，所以常用$\min_x\Theta_P(x)$来代表原始问题，定义原始问题的最优值： p^*=\min_x\Theta_P(x)​ 原始问题讨论到这里，总结：重新定义一个无约束问题，这个无约束问题等价于原来的约束优化问题。 对偶问题​ 定义关于$\alpha,\beta$的函数： \Theta_D(\alpha,\beta)=\min_xL(x,\alpha,\beta)​ 注意上面等式右边是关于$x$的函数最小化,$x$确定之后，最小值就只与$\alpha,\beta$有关，所以此时是一个关于$\alpha,\beta$的函数 ​ 再考虑极大化$\Theta_D(\alpha,\beta)=\min_xL(x,\alpha,\beta)$，即： \max_{\alpha,\beta:\alpha_i\ge0}\Theta_D(\alpha,\beta)=\max_{\alpha,\beta:\alpha_i\ge0}\min_xL(x,\alpha,\beta)​ 这就是原始问题的对偶问题，再将原始问题写出来： \min_x\Theta_P(x)=\min_x\max_{\alpha,\beta:\alpha_i\ge0}L(x,\alpha,\beta)=min_xf(x)​ 从形式上可以看出堆成，只不过原始问题先固定$L(x,\alpha,\beta)$中的$x$,优化参数$\alpha,\beta$，再优化$x$；而对偶问题是先固定$\alpha,\beta$，再优化$x$，然后再确定参数$\alpha,\beta$ ​ 定义对偶问题的最优值： d^*=\max_{\alpha,\beta:\alpha_i\ge0}\Theta_D(\alpha,\beta)原始问题与对偶问题之间的关系​ 若原始问题和对偶问题都有最优值，则有$Min-Max$不等式： d^*=\max_{\alpha,\beta:\alpha_i\ge0}\min_xL(x,\alpha,\beta)\le\min_x\max_{\alpha,\beta:\alpha_i\ge0}L(x,\alpha,\beta)=p^*​ 当$L(x,\alpha,\beta)$对$x$为凸函数，对$\alpha,\beta$为凹函数，以上等号成立。而$L(x,\alpha,\beta)$对$\alpha,\beta$为天然凹函数，因此只要$L(x,\alpha,\beta)$对$x$为凸函数，等号便成立 ​ 证明：一个式子的最大值永远大于等于这个式子的最小值，哪怕是这个式子最小的最大值与最大的最小值相比（瘦死的骆驼比马大）。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>拉格朗日对偶</tag>
        <tag>数学</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SVM与LR的异同]]></title>
    <url>%2F2018%2F02%2F19%2FSVM%E4%B8%8ELR%E7%9A%84%E5%BC%82%E5%90%8C%2F</url>
    <content type="text"><![CDATA[本篇文章主要是对几篇blog以及知乎回答的总结 [TOC] 相同点 1. LR和SVM都是有监督分类算法2. 不考虑核函数，LR和SVM都是线性分类算法他们的分类决策面都是线性的。 3. LR和SVM都是判别模型不同点 1. LR和SVM本质不同来自于$Loss Function$ LR是$logistical loss$: J(\theta)=-\sum_iy_i\log(p_{\theta}(x_i))+(1-y_i)\log(1-p_{\theta}(x_i))SVM是$hinge loss$： \sum^N_{i=1}[1-y_i(wx_i+b)]_++\lambda||w||^2 不同的$loss function$代表了不同的假设前提，也就代表了不同的分类原理 LR方法基于概率理论，假设样本满足$logistic$分布，然后通过极大似然估计的方法估计出参数的值 SVM基于几何间隔最大化原理，认为存在的最大几何间隔的分类面为最优分类面 所以，SVM只考虑分类面上的点（支持向量），而LR考虑所有点，在SVM中，在支持向量之外添加或者减少任何点都对结果没有影响，而LR每个点都会影响决策。 $Linear SVM$不直接依赖于数据分布，分类平面不受一类点影响 $LR$则是受所有点影响，所以受数据本身分布影响，如果数据不同类别$strongly unbalance$，一般需要先对数据$balancing$ 两种$loss function$目的都是增加对分类影响比较大的点的数据点权重，减少与分类关系比较小的数据点的权重。 $SVM$的处理方法是只考虑$support vectors$，也就是和分类最相关的少数点，去学习分类器。 $LR$则通过非线性映射，大大减小了离分类平面较远的点的权重，提高了与分类最相关的数据点的权重。（$sigmoid$函数中间比较陡峭？） 2. LR可以产生概率，SVM不能产生概率 LR模型的原理就是基于伯努利分布的假设推导出来的（见LR文章），所以它产生的结果代表了分成某一类的概率 SVM不是基于概率的假设，无法输出概率 3. SVM依赖数据的测度，而LR则不受影响 SVM是基于距离的，所以受测度影响；LR是基于概率的，所以不受测度影响 SVM需要最小化${1\over 2}||w||^2$，所以其依赖于不同维度的测度不同，如果差别较大需要做$normalization$，当然如果LR要加上正则化时，也是需要$normalization$ 如果用到梯度下降算法，则一般都需要$feature scaling$，如果不归一化，梯度下降会很慢 4. SVM自带结构风险最小化，LR不带正则项则是经验风险最小化 因为SVM本身就是优化${1\over 2}||w||^2$最小化的，所以其优化的目标函数本身就含有结构风险最小化，所以不需要加正则项 LR不加正则化的时候，优化目标就是经验风险最小化，所以最后需要加入正则项，增强模型泛化能力 5. SVM会用核函数而LR一般不用核函数的原因 SVM转化为对偶问题后，分类只需要计算与少数个支持向量的距离（原因从KKT条件最后一条理解，非支持向量不提供约束），这个在进行复杂核函数计算时优势很明显，能够大大简化模型和计算量 LR则是每个点都需要两两计算核函数，计算量庞大 6. LR和SVM在实际应用中的区别 根据经验来看，对于小规模数据集，SVM的效果要好于LR，但是在大数据中，SVM计算复杂度收到限制，而LR因为训练简单，可以在线训练，所以经常被大量采用。 $Andrew NG$在课里讲过： 如果$Feature$的数量很大，这时候选用$LR$或者是$Linear Kernel$的$SVM$ 如果$Feature$的数量较小，数据量中等，用$Gaussian Kernel$的$SVM$ 如果$Feature$的数量较小，数据量比较大，构造特征，然后同第一种 参考链接： 止战blog 【机器学习】Linear SVM 和 LR 的联系和区别 SVM和logistic回归分别在什么情况下使用？]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>LR</tag>
        <tag>SVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[支持向量机SVM]]></title>
    <url>%2F2018%2F02%2F18%2F%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BASVM%2F</url>
    <content type="text"><![CDATA[线性可分支持向量机与硬间隔最大化 线性可分支持向量机考虑一个二分类问题，假设输入空间和特征空间为两个不同的空间。输入空间为欧式空间或离散集合，特征空间为欧式空间或希尔伯特空间。线性可分支持向量机、线性支持向量机假设这两个空间的元素一一对应，并将输入空间中的输入映射为特征空间中的特征向量。非线性支持向量机利用一个从输入空间到特征空间的非线性映射将输入映射为特征向量。所以输入都有输入空间转换到特征空间，支持向量机的学习是在特征空间进行的。 定义 线性可分支持向量机：给定线性可分训练数据集，通过间隔最大化或等价地求解相应的凸二次规划问题学习得到的分离超平面为： w^￥x+b^￥=0以及相应的分类决策函数： f(x)=sign(w^￥x+b^￥)函数间隔和几何间隔一般来说，一个点距离分离超平面的远近可以表示分类预测的确信程度。在超平面$wx+b=0$确定的情况下，$|wx+b|$能够相对的表示点$x$距离超平面的远近。而$wx+b$的符号与类标记$y$的符号是否一致能够表示分类是否正确。所以可用量$y(wx+b)$来表示分类的正确性以及确信度，这就是函数间隔的概念。 为什么$|wx+b|$是相对的表示距离：首先这指的是函数距离，同一个函数内有比较价值。其次因为$b$的影响，真正距离并不是这个，但是距离的比较都有这个$b$，所以这里就不再考虑 定义 函数间隔：对于给定的训练数据集$T$和超平面$(w,b)$，定义超平面$(w,b)$关于样本点$(x_i,y_i)$的函数间隔为 \hat\gamma_i=y_i(wx_i+b)定义超平面$(w,b)$关于训练数据集$T$的函数间隔为超平面$(w,b)$关于$T$中所有样本点$(x_i,y_i)$的函数间隔之最小值（注意这里是指到平面最小距离的点的距离，其实就是支持向量到分离超平面的距离），即： \hat\gamma=\min_{i=1,…,N}\hat\gamma_i选择分离超平面时，只有函数间隔还不够，因为只要成比例的改变$w$和$b$，例如将它们改写为$2w$和$2b$，超平面并没有改变，但函数间隔却成为原来的$2$倍。所以可以对分离超平面的法向量$w$加某些约束，如规范化，$||w||=1$，使得间隔是确定的。这时函数间隔成为几何间隔。 定义 几何间隔：对于给定的训练数据集$T$和超平面$(w,b)$，定义超平面$(w,b)$关于样本点$(x_i,y_i)$的几何间隔为： \gamma_i=y_i({w\over ||w||}x_i+{b\over||w||})定义超平面$(w,b)$关于训练数据集$T$的函数间隔为超平面$(w,b)$关于$T$中所有样本点$(x_i,y_i)$的几何间隔之最小值（注意这里是指到平面最小距离的点的距离，其实就是支持向量到分离超平面的距离），即： \gamma=\min_{i=1,…,N}\hat\gamma_i函数间隔和几何间隔的关系： \gamma={\hat\gamma\over||w||}\ \ \ \ \ (1)如果$||w||=1$，那么函数间隔和几何间隔相等。如果超平面参数$w$和$b$成比例的改变（超平面没有改变），函数间隔也按此比例改变，而几何间隔不变。 间隔最大化间隔最大化的直观解释是：对训练数据集找到几何间隔最大的超平面意味着一充分大的确信度对训练数据进行分类。也就是说，不仅将正负实例点分开，而且对最难分的实例点（离超平面最近的点）也有足够大的确信度将它们分开。这样的超平面应该对未知的新实例有很好的分类预测能力。 最大间隔分离超平面求一个几何间隔最大的分离超平面可以表示为下面问题的最优化： \max_{w,b}\ \ \ \gammas.t.\ \ y_i({w\over||w||}x_i+{b\over||w||})\ge\gamma,\ \ i=1,2,…,N我们希望最大化几何间隔$\gamma$（$\gamma$为所有点的中几何距离最短的点的几何距离，就是支持向量的几何距离），约束条件表示的是超平面关于每个训练样本点的几何间隔至少是$\gamma$。 考虑几何间隔与函数间隔的关系式$(1)$，可以将这个问题改写为(这里为了构造出$||w||$): \max_{w,b}\ \ \ {\hat\gamma\over||w||}s.t.\ \ y_i(wx_i+b)\ge\hat\gamma,\ \ i=1,2,…,N**函数间隔$\hat\gamma$的取值并不影响最优化问题的解。将$w,b$按比例变化为$\lambda w,\lambda b$，超平面并未发生改变，函数间隔变为$\lambda\hat\gamma $。所以函数间隔的改变只影响$w,b$的系数，不改变超平面。这样就可以去$\hat\gamma=1$，并带入以上最优化问题。注意到最大化$1\over ||w||$和最小化${1\over 2}||w||^2$是等价地。于是就得到了下面线性可分支持向量机学习的最优化问题： \min_{w,b}\ \ \ {1\over 2}||w||^2s.t.\ \ \ y_i(wx_i+b)-1\ge0,\ \ \ i=1,2,…,NSVM部分暂时弃更，东西差不多都是《统计学习方法上的》，关于自己理解的部分也都标注在了书上，手码实在太多了，这部分先看书把。。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>SVM</tag>
        <tag>支持向量机</tag>
        <tag>核函数</tag>
        <tag>拉格朗日</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[最大熵模型]]></title>
    <url>%2F2018%2F02%2F17%2F%E6%9C%80%E5%A4%A7%E7%86%B5%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[最大熵模型($The Maximum Entropy$，从信息论的角度来讲，就是保留了最大的不确定性，也就是让熵达到最大。当我们需要对一个时间的概率分布进行预测是，最大熵原理告诉我们所有的预测应当满足全部已知的条件，而对未知的情况不要做任何主观假设（不做主观假设这点很重要）。也就是让概率分布最均匀，预测风险最小。 数学知识熵熵（$Entropy$）是热力学中的概念，由香农引入到信息论中。在信息论和概率统计中，熵用来表示随机变量不确定性的度量。 定义：设$X\in\{x_1,x_2,x_3…,x_n\}$为一个离散随机变量，其概率分布为$p(X=x_i)=p_i, i=1,2,…,n$，则$X$的熵为 H(X)=-\sum^n_{i=1}p_i\log p_i其中，当$p_i=0$时，定义$0\log 0=0.$ 注意：$H(x)$依赖于$X$的分布，而与$X$的具体值无关。$H(x)$越大，表示$X$的不确定性越大。 条件熵定义：设$X\in\{x_1,x_2,x_3…,x_n\}, Y\in \{y_1,y_2,…,y_m\}$为离散随机变量，在已知$X$的条件下，$Y$的条件熵$(Conditional Entropy)$可定义为 H(Y|X)=\sum^n_{i=1}H(Y|X=x_i)=-\sum^n_{i=1}p(x_i)\sum^m_{j=1}p(y_j|x_i)\log p(y_j|x_i)它表示已知$X$的条件下，$Y$的条件概率分布的熵对$X$的数学期望。 最大熵原理最大熵原理是概率模型学习的一个准则，最大熵原理认为，学习概率模型时在所有可能的概率模型（分布）中，熵最大的模型是最好的模型。通常用约束条件来确定概率模型集合，所以，最大熵原理也可以表述为在满足约束条件的模型集合中选取熵最大的模型。 假设离散随机变量$X$的概率分布是$P(X)$，则其熵是 H(P)=-\sum_xP(x)\log P(x)熵满足下列不等式： 0\le H(P)\le \log |X|式中，$|X|$是$X$的取值个数，当且仅当$X$的分布式均匀分布时右边的等号成立。这就是说，当$X$服从均匀分布时候，熵最大。 最大熵原理认为要选择的概率模型首先必须满足已有的事实，即约束条件。在没有更多信息条件下，那些不确定的成分是‘等可能的’。最大熵原理通过熵最大化来表示等可能性。‘等可能性’不易操作，而熵则是可优化的数值指标。 最大熵模型定义前提1.最大熵原理是统计学习的一般原理，将它应用到分类就得到了最大熵模型 2.假设分类模型是一个条件概率分布$P(Y|X),$$X$表示输入，$Y$表示输出。这个模型表示的是对于给定的输入$X$，一条件概率$P(Y|X)$输出$Y$。 3.给定一个训练数据集$T$，我们的目标就是利用最大熵原理选择最好的分类模型。 T=\{(x_1,y_1),(x_2,y_2),…(x_N,y_N)\}4.按照最大熵原理，我们应该优先保证模型满足一致的所有约束。那么如何得到这些约束的？ $ $思路：从训练数据$T$中抽取若干特征（依靠特征函数），然后要求这些特征在$T$上关于经验分布的期望与他们在模型中关于$p(x,y)$的数学期望相等，这样，一个特征就对应一个约束。 特征函数特征函数的作用1.用特征函数$f(x)$描述输入$x$和输出$y$之间的某一个事实。 2.按照最大熵原理，应该优先保证模型满足一致的所有约束。通过特征函数来定义、量化以及得到这些约束。 3.特征函数的出现，可以让模型有更好的泛化能力（特征函数的选取、定义特别随意）。 特征函数可以类比决策树对于输入$x$的处理，假设输入$x$是一个有着多个属性值的实例，决策树在一个决策点并不是对所有属性都进行考虑，这就有点提取出了特征中更有信息量的属性。 如何让一个线性模型（例如$LR:h_{\theta}(x)=\sigma (\theta^Tx)$ ）也有类似的功能？答案就是特征函数，让输入$X$先经过一些列特征函数的处理，变成$g(x)$再送给模型分类（如：$h_{\theta}(x)=\sigma (\theta^Tg(x))$）. 此外，当输入的样本可能不是数值的向量，比如文本或图片时，特征函数的功能更像是特征向量的制作。对于给定输入$X$，使用一系列定义好的特征函数$\{g(x)\}$将其转换成需要的向量形式。 对于特征函数中‘特征’的理解一般说的“特征”都是指输入的特征，而最大熵模型中的“特征”指的是输入和输出共同的特征。最大熵模型中的每个特征会有一个权重，你可以把它理解成这个特征所描述的输入和输出有多么倾向于同时出现。 可以以多类logistic regression为例，来感受一下两种视角的不同。在一般的视角下，每条输入数据会被表示成一个n维向量，可以看成n个特征。而模型中每一类都有n个权重，与n个特征相乘后求和再经过softmax的结果，代表这条输入数据被分到这一类的概率。在最大熵模型的视角下，每条输入的n个“特征”与k个类别共同组成了nk个特征，模型中有nk个权重，与特征一一对应。每个类别会触发nk个特征中的n个，这n个特征中的每个特征都会触发特征函数。 经验分布经验分布是指通过训练数据$T$进行统计得到的分布。我们需要考察两个经验分布，分别是$x,y$的联合经验分布，以及$x$的经验分布。其定义如下： \hat p(x,y)={count(x,y)\over N},\ \hat p(x)={count(x)\over N}对于任意特征函数$f$,记$E_{\hat p}(f)$表示$f$在训练数据$T$上关于$\hat p(x,y)$的数学期望。$E_p(f)$表示$f$在模型上关于$p(x,y)$的数学期望，由于模型中$p(x,y)$是未知的，并且我们建模的目标是$p(y|x)$，因此我们利用$Bayes$定理得到$p(x,y)=p(x)p(y|x)$，此时，$p(x)$还是未知的，我们可以使用经验分布$\hat p(x)$对$p(x)$进行近似。按照期望的定义有： E_{\hat p}(f)=\sum_{x,y}\hat p(x,y)f(x,y)E_p(f)=\sum_{x,y}p(x,y)f(x,y)=\sum_{x,y}\hat p(x)p(y|x)f(x,y)对于概率分布$p(y|x)$我们希望特征$f$的期望应该和从训练数据中的到的一样的。（我的理解：特征函数是约束的提取量化，特征函数关于经验分布的期望与关于模型的期望相等就代表一个约束）因此我们可以提出约束： E_{\hat p}(f)=E_p(f)\sum_{x,y}\hat p(x,y)f(x,y)=\sum_{x,y}\hat p(x)p(y|x)f(x,y)假设从训练数据中抽取了n个特征，相应的便有n个特征函数以及n各约束条件。 C_i:E_p(f_i)=E_{\hat p}(f_i):=\tau_i,i=1,2,…,n最大熵模型给定数据集$T$，我们的目标就是根据最大上原理选择一个最优分类器，假设满足所有约束条件的模型集合为： C=\{P\in\pi|E_p(f_i)=E_{\hat p}(f_i),i=1,2,…,n \}定义在条件概率分布$P(Y|X)$上的条件熵为： H(P)=-\sum_{x,y}\hat P(x)P(y|x)\log P(y|x)则模型集合$C$中条件熵$H(P)$最大的模型成为最大熵模型。式中的对数为自然对数。 最大熵模型的学习最大熵模型的学习可以形式化为约束最优化问题。对于给定训练数据集$T=\{(x_1,y_1),(x_2,y_2),…,(x_n,y_n)\}$以及特征函数$f_i(x,y), i=1,2,3,…,n$，最大熵模型的学习等价于约束最优化问题(注意自变量为$P$)： \max_{P\in C}\ \ \ H(P)=-\sum_{x,y}\hat P(x)P(y|x)\log P(y|x)\ \ \ s.t.\ \ \ E_P(f_i)=E_{\hat P}(f_i),\ i=1,2,….,n\sum_y P(y|x)=1\ \ \ \ \ \ \ \ \ \ \ \按照最优化习惯，将求最大值问题改写为等价的求最小值问题： \min_{P\in C}\ \ \ -H(P)=\sum_{x,y}\hat P(x)P(y|x)\log P(y|x)\ \ \ s.t.\ \ \ E_P(f_i)-E_{\hat P}(f_i)=0,\ i=1,2,….,n1-\sum_y P(y|x)=0\ \ \ \ \ \ \ \ \ \ \ \引入拉格朗日乘子$w_0,w_1,w_2,…,w_n$,定义拉格朗日函数$L(P,w)$(这里注意两自变量)： L(P,w)=-H(p)+w_0(1-\sum_yP(y|x))+\sum^n_{i=1}w_i(E_{\hat P}(f_i)-E_P(f_i))又： -H(P)=\sum_{x,y}\hat P(x)P(y|x)\log P(y|x)E_{\hat p}(f)=\sum_{x,y}\hat p(x,y)f(x,y)E_p(f)=\sum_{x,y}\hat p(x)p(y|x)f(x,y)带入得到$L(P,w)$: L(P,w)=\sum_{x,y}\hat P(x)P(y|x)\log P(y|x)+w_0(1-\sum_yP(y|x)\ )+\sum ^n_{i=1}w_i(\sum_{x,y}\hat p(x,y)f(x,y)-\sum_{x,y}\hat p(x,y)f(x,y)\ )对偶最优化的原始问题是（再次注意$min,max$的自变量是什么）： \min_{p\in C}\max_wL(P,w)对偶问题是 \max_w\min_{p\in C}L(P,w)由于拉格朗日函数$L(P,w)$是$P$的凸函数，原始问题的解与对偶问题的解是等价的。这样，可以通过求解对偶问题来求解原始问题 注：以上的推理解释可在另一篇拉格朗日文章中找到 首先，求解对偶问题内部的极小化问题$\min_{p\in C}L(P,w)$。$\min_{p\in C}L(P,w)$是关于$w$的函数（因为$\min_{p\in C}L(P,w)$代表$P$已经确定了），将其记作： \Psi(w)=\min_{P\in C}L(P,w)=L(P_w,w),注：上式的最后一项$L(P_w,w)$是归于$w$的函数，因为此时$P_w=\arg min_{P\in C}L(P,w)=P_w(y|x)$ $\Psi(x)$成为对偶函数，其解为$P_w$。 具体地，求$L(P,w)$对$P(y|x)$的偏导数（这里直接插入图片了手写这一段$mathjax$太累，将图片中$\lambda$换成$w$就行）注意蓝色字部分 另偏导数等于0，在$\hat P(x)&gt;0$的情况下，解得： P(y|x)=exp(\ \sum^n_{i=1}w_if_i(x,y)+w_0-1\ )={exp(\sum^n_{i=1}w_if_i(x,y))\over \exp(1-w_0)}由于$\sum_y P(y|x)=1$，得(关于$w$的函数)： P_w(y|x)={1\over Z_w(x)}exp(\ \sum^n_{i=1}w_if_i(x,y)\ )其中， Z_w(x)=\sum_yexp(\ \sum^n_{i=1}w_if_i(x,y)\ )$Z_w(x)$成为规范化因子；$f_i(x,y)$是特征函数；$w_i$是特征的权值。表示的模型$P_w=P_w(y|x)$就是最大熵模型。这里$w$是最大熵模型中的参数向量。 得到对偶问题内部的极小问题的解$P_w(y|x)$后，需要进一步求解外层的极大值问题。 \max_w\Psi(w)将其解记为$w^￥$,即： w^￥=\arg\ max_w\Psi(w)令$\sum_{x,y}\hat P(x,y)f_i(x,y) = \tau_i$ $ \Psi (x)$ $=L(P_w,w) 注意这里P_w为定量是之前内部极小问题的解$ $=\sum_{x,y}\hat P(x)P_w(y|x)\log P_w(y|x)+\sum^n_{i=1}w_i( \tau_i-\sum_{x,y}\hat P(x)P_w(y|x)f_i(x,y) )$ $=\sum^n_{i=1}w_i\tau_i+\sum_{x,y}\hat P(x)P_w(y|x)( \log p_w(y|x)-\sum^n_{i=1}w_if_i(s,y) )$ 又:$\log P_w(y|x)=\sum^n_{i=1}w_if_i(x,y)-\log Z_w(x)$ 将上式带入到$\Psi$中，可以得到 $\Psi(w)$ $=\sum^n_{i=1}w_i\tau_i-\sum_{x,y}\hat P(x)P_w(y|x)\log Z_w(x)$ $=\sum^n_{i=1}w_i\tau_i-\sum_{x}\hat P(x)\log Z_w(x)\sum_y P_w(y|x)$ $=\sum^n_{i=1}w_i\tau_i-\sum_{x}\hat P(x)\log Z_w(x) (这里利用了\sum_yP_w(y|x)=1 )$ 极大似然模型下面证明对偶函数的极大化等价于最大熵模型的极大似然估计。 注：极大似然估计$MLE$的一般形式表示为（推导下一小节给出）： L_{\hat P}=\Pi_xP(x)^{\hat P(x)}一直训练数据的经验概率分布$\hat P(x,y)$，条件概率分布$P(Y|X)$的对数似然表示为： L_{\hat P}(P_w)=\log \Pi_{x,y}P(y|x)^{\hat P(x,y)}=\sum_{x,y}\hat P(x,y)\log P(y|x)当条件概率分布$P(y|x)$是最大熵模型的内部极小函数的解释，对数似然函数$L_{\hat P}(P_w)$ 为： 注：最后一步由$\sum_{x,y}\hat p(x,y)=\sum_x\hat p(x)\sum_yp(y|x)$且$\sum_yp(y|x)=1$得来 最大熵模型中对数似然的解释转载自CSDN 最近在学习最大熵模型，看到极大似然估计这部分，没有看明白条件概率分布$p(y|x)$的对数似然函数。上网查了很多资料都没有一个合理的解释。基本直接给出对数似然函数的一般形式 ($\hat p(x)$为jing’yan’gai’lv): L\hat p=\Pi_xp(x)^{\hat p(x)}其实第一眼之所以不理解，因为这是最大似然函数的另外一种形式。一般书上描述的最大似然函数的一般形式是各个样本集XX中各个样本的联合概率: L(x_1,x_2,…,x_n)=\Pi^n_{i=1}p(x_i;\theta)其实这个公式和上式是等价的。$x_1,x_2,…,x_n$是样本具体观测值。随机变量$X$是离散的，所以它的取值范围是一个集合，假设样本集的大小为$n$，$X$的取值有$k$个，分别是$v_1,v_2,…,v_k$。用$C(X=v_i)$表示在观测值中样本$v_i$出现的频率。所以$L(x_1,x_2,…,x_n;θ)$可以表示为： L(x_1,x_2,…,x_n)=\Pi^k_{i=1}p(v_i;\theta)^{C(X=v_i)}对等式两边同时开$n$次方得： L(x_1,x_2,…,x_n)^{1\over n}=\Pi^k_{i=1}p(v_i;\theta)^{C(X=v_i)\over n}因为经验概率$\hat p(x)={C(X=v_i)\over n}$，所以简写可以得到 L(x_1,x_2,…,x_n)^{1\over n}=\Pi_xp(x;\theta)^{\hat p(x)}很明显对$L(x_1,x_2,…,x_n;θ)$求最大值和对$L(x_1,x_2,…,x_n;θ)^{1\over n}$求最大值的优化的结果是一样的。整理上式所以最终的最大似然函数可以表示为： L(x;\theta)=\Pi_xp(x;\theta)^{\hat p(x)}从最大熵模型角度理解LRLR是最大熵模型在类别为2时候的特例 假设每条输入第$i$个特征对第$k$类的贡献是$w_{ki}$，则数据点$(x_1,x_2,…,x_n)$属于第$k$类的概率正比于$exp(w_{k1}x_1+w_{k2}x_2+…+w_{kn}x_n)$。 f(x,y) =\begin{cases}x_i, & \text{y=1}\ \ \ \ i=1,2,…,n \\0, & \text{y=0}\end{cases}根据最大熵模型： P(y=k)={\exp (\sum^n_{i=1}w_{ki}x_i)\over \sum_y\exp (\sum^n_{i=1}w_{yi}x_i)}现在回到两类的情况$\{0,1\}$，此时分母上有两项： P(y=1)={\exp (\sum^n_{i=1}w_{1i}x_i)\over \exp (\sum^n_{i=1}w_{1i}x_i)+\exp (\sum^n_{i=1}w_{0i}x_i)}分子、分母同时除以分子，则有： P(y=1)={1\over 1+\exp(-\sum^n_{i=1}w_ix_i)}这就变成了$logistic$函数。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>LR</tag>
        <tag>最大熵模型</tag>
        <tag>数学</tag>
        <tag>特征函数</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Logistic Regression的理解]]></title>
    <url>%2F2018%2F02%2F16%2FLogistic-Regression%E7%9A%84%E7%90%86%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[逻辑斯地回归模型逻辑斯谛分布设$X$是连续随机变量，$X$服从逻辑斯谛分布是指$X$具有下列分布函数和密度函数： F(x)=P(X\le x)= {1 \over 1+e^{-(x-\mu )/\gamma}}f(x)=F'(x)={e^{-(x-\mu)/y} \over \gamma(1+e^{-(x-\mu)}/\gamma)^2}式中，$\mu$为位置参数，$\gamma\gt0$为形状参数。密度函数与分布函数如下图所示。 注：逻辑斯谛分布的回归会在从广义线性模型角度理解LR时给出 二项逻辑斯谛回归模型二项逻辑斯谛回归模型是一种分类模型，由条件概率分布$P(Y|X)$表示，形式为参数化的逻辑斯谛分布。这里，随机变量取值为实数，随机变量Y，取值为1或0，即Y满足二项分布。 定义：二项逻辑斯谛回归模型是如下的条件概率分布： P(Y=1|x)={exp(wx) \over 1+exp(wx) }={1\over 1+exp(-wx)}P(Y=0|x)={1 \over 1+exp(wx)}={exp(-wx) \over 1+exp(-wx)}一个事件的几率（odds）是指该事件发生的概率与该事件不发生的概率的比值。如果事件发生的概率是$p$,那么该事件的几率是${p\over 1-p}$，该事件的对数几率或者logit函数是： logit(p)=\log {p \over 1-p}这里$logit(p)$就是广义线性模型中的$\eta$。 对于逻辑斯谛回归而言 \log {P(Y=1|x) \over 1-P(Y=1|x)}=wx\ \ \ 其实就是广义线性模型第三条假设从广义线性模型角度，二项分布指数族解出的$\eta = \log{p\over 1-p}=logit(p)$,又由第三条假设，$\eta=wx$即可得到$\log {p\over 1-p}=wx$,求解出$p$来就是$P(Y=1|X)$的概率，又因为二项分布期望为$P$,所以收敛的结果就是$P$. 模型参数估计逻辑斯谛回归模型学习中，对于给定的训练数据集$T=\{(x_1,y_1),(x_2,y_2),…,(x_N,y_n)\}$ ，其中，$x_I\in R^n,y_i\in\{0,1\}$,可以应用极大似然估计法估计模型参数，从而得到逻辑斯谛回归模型。设： P(Y=1|x)=\pi(x),\ \ \ P(Y=0|x)=1-\pi(x)似然函数为 \Pi^N_{i=1}[\pi(x_i)^{y_i}][1-\pi(x)]^{1-y_i}对数似然函数为 L(w)=\sum^N_{i=1}[y_i\log \pi(x_i)+(1-y_i)\log (1-\pi(x_i))]\ \ \ =\sum^N_{i=1}[y_i\log{\pi(x) \over 1-\pi(x_i)}+\log(1-\pi(x_i))]\ \ =\sum^N_{i=1}[y_i(wx)-\log(1+exp(wx))]\ \ \ \ \ \ \从最大熵模型角度理解LRLR是最大熵模型在类别为2时候的特例 假设每条输入第$i$个特征对第$k$类的贡献是$w_{ki}$，则数据点$(x_1,x_2,…,x_n)$属于第$k$类的概率正比于$exp(w_{k1}x_1+w_{k2}x_2+…+w_{kn}x_n)$。 f(x,y) =\begin{cases}x_i, & \text{y=1}\ \ \ \ i=1,2,…,n \\0, & \text{y=0}\end{cases}根据最大熵模型： P(y=k)={\exp (\sum^n_{i=1}w_{ki}x_i)\over \sum_y\exp (\sum^n_{i=1}w_{yi}x_i)}​现在回到两类的情况$\{0,1\}$，此时分母上有两项： P(y=1)={\exp (\sum^n_{i=1}w_{1i}x_i)\over \exp (\sum^n_{i=1}w_{1i}x_i)+\exp (\sum^n_{i=1}w_{0i}x_i)}分子、分母同时除以分子，则有： P(y=1)={1\over 1+\exp(-\sum^n_{i=1}w_ix_i)}这就变成了$logistic$函数。 从广义线性模型角度理解LR线性回归中我们假设： y(x;\theta) 服从N(\mu,\sigma^2)分布$LR$中我们假设： y(x;\theta)服从Bernoulli(\phi)分布其实他们只是广义线性模型($GlMs$)的特例。 自己的理解广义线性模型是通过链接函数（$LR$中为$logit$函数），把自变量的线性组合（$\eta$ 自然参数/标准参数）与因变量（$T(y)$)的期望联系起来。 注：$LR$也可以说与因变量的概率分布结合起来，因为二项伯努利分布$E=P$ 指数分布族（$The exponential family$）首先我们定义一下什么是指数分布族，它有如下形式($\eta$自变量，$y$因变量)： p(y;\eta)=b(y)\exp(\eta^TT(y)-a(\eta))简单介绍一下其中的参数： 1.$\eta $是自然参数 2.$T(y)$是充分统计量(一般情况下$T(y)=y$) 3.$a(\eta)$是$\log partition function$( $ exp(-a(\eta))$充当正规化常量的角色，保证$\sum p(y;\eta)=1 $) 也就是说$T,a,b$确定了一种分布，$\eta$是该分布的参数。 选择合适的$T,a,b$我们可以得到高斯分布和$Bernouli$分布 广义线性模型的形式化定义GLM有三个假设： 1.$y|x;\theta$~$ExpFamily(\eta)$（某指数分布族）；给定样本$x$与参数$\theta$，样本分类$y$服从指数分布族中的某个分布； 2.给定一个$x$，我们需要的目标函数为$h_{\theta}(x)=E[T(y)|x]$ 3.$\eta=\theta^Tx$ 推导过程]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>LR</tag>
        <tag>最大熵模型</tag>
        <tag>广义线性回归</tag>
        <tag>逻辑斯谛回归</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[广义线性模型]]></title>
    <url>%2F2018%2F02%2F16%2F%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[广义线性模型概要本文将会说明线性回归和$LR$都是广义线性模型的一种特殊形式，介绍广义线性模型的一般求解步骤。 线性回归中我们假设： y(x;\theta) 服从N(\mu,\sigma^2)分布$LR$中我们假设： y(x;\theta)服从Bernoulli(\phi)分布其实他们只是广义线性模型($GlMs$)的特例。 自己的理解广义线性模型是通过链接函数（$LR$中为$logit$函数），把自变量的线性组合（$\eta$ 自然参数/标准参数）与因变量（$T(y)$)的期望联系起来。 注：$LR$也可以说与因变量的概率分布结合起来，因为二项伯努利分布$E=P$ 指数分布族（$The exponential family$）首先我们定义一下什么是指数分布族，它有如下形式($\eta$自变量，$y$因变量)： p(y;\eta)=b(y)\exp(\eta^TT(y)-a(\eta))简单介绍一下其中的参数： 1.$\eta $是自然参数 2.$T(y)$是充分统计量(一般情况下$T(y)=y$) 3.$a(\eta)$是$\log partition function$( $ exp(-a(\eta))$充当正规化常量的角色，保证$\sum p(y;\eta)=1 $) 也就是说$T,a,b$确定了一种分布，$\eta$是该分布的参数。 选择合适的$T,a,b$我们可以得到高斯分布和$Bernouli$分布 广义线性模型的形式化定义GLM有三个假设： 1.$y|x;\theta$~$ExpFamily(\eta)$（某指数分布族）；给定样本$x$与参数$\theta$，样本分类$y$服从指数分布族中的某个分布； 2.给定一个$x$，我们需要的目标函数为$h_{\theta}(x)=E[T(y)|x]$ 3.$\eta=\theta^Tx$ 高斯分布的另一种看法 伯努利分布-LR回归的含义]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>LR</tag>
        <tag>理论</tag>
        <tag>广义线性模型</tag>
        <tag>机器学习</tag>
        <tag>GLM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2017-JDD京东金融算法大赛12th解决方案]]></title>
    <url>%2F2018%2F01%2F15%2F2017-JDD%E4%BA%AC%E4%B8%9C%E9%87%91%E8%9E%8D%E7%AE%97%E6%B3%95%E5%A4%A7%E8%B5%9B12th%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88%2F</url>
    <content type="text"><![CDATA[赛题训练数据包含2017-04-30日之前270天之内若干店铺的每日订单量、销售额、顾客数、评价数、广告费用等数据，下架时间在2017-04-30之后或者未下架的商品数据，以及这些店铺2016年6月-2017年1月每月末后90天内的销售额。 通过竞赛数据中店铺过往的销售记录，商品信息，商品评价，以及广告费用等信息来建立预测模型，预测店铺未来90天内的销售额。 赛题分析 这是个时间序列问题，时间滑窗的选择是关键点。 数据是真实数据，难免会存在刷单、特殊节日、店铺停业等等特殊情况 不同店铺销量波动水平可能不同 特征的选择按照商品特征、订单特征、评价特征、交叉特征、销售额和下架分别构造特征 数据处理刷单行为通过DEA发现存在刷单行为，通过’&gt;本月销售中位数*10（根据线上选出的阈值）’判断删除该日订单 活动促销双十一和618（因为双11不存在最终训练集里面，所以不用考虑训练集的影响。但是6.18在预测集里面，这里考虑用双11修正618的销量。具体挑出11月份销量波动大的店铺，计算修正系数，预测完相应店铺称该修正系数） 特殊月份过年、情人节等，EDA发现影响不大，故不作考虑 下架商品某些店铺会在短时间内下架大量的商品 特征部分商品、订单、评价（取平均的时候是按照有效评论日期取平均）、交叉特征、销售以及下架特征。（主要以本月为主部分特征加入前几个月的相关统计特征） 后来发现销售额有关特征重要性很强，于是又添加了额外的钱有关的本月统计有关特征（本月后三、二、一周） 特征部分实在是无法回忆清楚了，这个比赛来来回回构建了许多类似特征，特征的增减也是根据线上表现大刀阔斧地一群一群增删。下面写一下大致思路，不是所有特征都构建三个月的统计特征，印象中总销售、总订单量之类的总量特征会添加上月、上上月、上上上月的总量统计特征以及三月平均特征。平均数特征方面几乎没有构造统计特征。 商品特征 在售总商品数 平均每个商品的订单量 平均每个商品的实际销量 平均每个商品的退货订单数 未售商品占总商品的比例 订单特征 总销售金额 平均每个订单销售金额 总优惠金额 平均每笔订单优惠金额 总优惠金额占总销售金额比 总订单量 总退货订单量 总实际订单量 总退货订单金额 退货金额占总销售额金额的比 总实际销售金额 平均每笔订单实际销售金额 总顾客数 平均每个顾客的订单量 平均每个顾客的购买金额 平均每个顾客的退货订单数 平均每个顾客的退货金额 总优惠笔数 总优惠金额占总退货金额比例 平均每笔订单总优惠金额占总退货金额比例 平均每笔优惠金额 平均销售金额增长率（每个月和前一个月算增长率，所有增长率取平均） 平均订单量增长率 平均退货订单增长率 平均退货金额增长率 评价特征（取平均的时候是按照有效评论日期取平均） 总好评数 总中评数 总差评数 总评论数 平均好评数 平均中评数 平均差评数 好评率 差评率 中评率 平均好评率增长率 交叉特征 平均每个月的充值广告费用占总销售金额比 平均每个月的充值广告费用占实际销售金额比 平均每个订单的好评率 平均每个订单的差评率 平均每个订单的好评数 平均每个订单的差评数 销售额和下架特征 总销售额 当月销售额 前一个月总销售金额 前两个月总销售金额 前三个月总销售金额 前一个周总销售金额 前两个周总销售金额 前三个周总销售金额 前一个月下架商品数 前两个月下架商品数 前三个月下架商品数 最近一周下架商品数 一开始加的特征比较多，因为效果还不错吧，所以也没有根据线上去判断一下哪些特征是否有用，而且这个比赛想构建一个比较稳定的线下验证是非常困难的，因为销量波动还是比较大的，所以到最后我还没构建出合理的线下验证集，更多的时候是以线上来验证我的一些想法。根据特征的重要性来看，跟销售金额有关的特征比较强一些，某些特征重要性非常低但是我也没删除。 模型训练这部分没什么可说的，就是这次比赛数据量较少，只有3000条，相对而言特征维度较高，模型着重调整了正则化部分，防止过拟合 后处理某些店铺销量变化巨大，所以想到通过方差筛选出波动比较大的店铺，该店铺在预测完成之后单独处理，规则是，把最后一个９０天，除以每家店铺上个月的销量，再将这个值取平均，结果是２.7左右，所以这个规则就是每个店最后一个月销量×２.7，这是规则基础版，同时可以根据每个月不同销量修改参数，例如（３月×0.2+4月×０.８）*2.7。 另外这次比赛不同模型预测结果差异很大，模型融合效果显著稳定上分。 $0.2lgb+0.8xgb$]]></content>
      <categories>
        <category>比赛</category>
      </categories>
      <tags>
        <tag>大数据竞赛</tag>
        <tag>JDD</tag>
        <tag>销量预测</tag>
        <tag>时间序列</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2017 CCF 大数据竞赛Top4%]]></title>
    <url>%2F2018%2F01%2F13%2F2017-CCF-%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%AB%9E%E8%B5%9BTop4%2F</url>
    <content type="text"><![CDATA[2017 CCF 大数据比赛记录（首赛）此次是在CCF平台与天池平台联合举办的大数据比赛，题目是：商场中精确定位用户所在店铺，主办方是蚂蚁金服。我和朋友两个人都是第一次参加这种大数据比赛，也算一步一个坑走了过来…线下赛最终经过反作弊筛选后是前100，共有2845支队伍。最后因为我俩临近期末考试，比赛也就就此结束了。 赛题给出用户在商场使用手机支付时所采集到的信息，包括用户信息，店铺信息，商场信息等，要求预测给出上述信息后精准预测用户所在店铺。具体给出的数据表可以点击这里来看。 赛题分析 问题初看是多分类问题，但是如果直接处理的话$label$上千个。仔细观察题目可以发现， 不同商场里面的商店相互独立，所以这里考虑为每一个商场建立一个独立的预测模型。 WiFi信息信息量最大，为主要信息。但是所给WiFi信息繁杂不规范，给WiFi信息找到合理的处理模式至关重要（WiFi指纹）。 如何挖掘WiFi隐含信息，并建立其与$label$之间的关联（具体WiFi id、信号强度、连接与否）至关重要。 WiFi信息中有比较稳定的WiFi（商店自己的WiFi），有用户的个人热点，还有各种公共WiFi（CMCC之类的）。这里讲后两者当做是不稳定WiFi，或者说是噪音WiFi，预测时过滤掉 许多记录WiFi信息缺失，将此作为异常值直接删除（因为总数据量大，单条数据影响不大，WiFi信息又为最重要的信息） 不同商铺的经纬、WiFi信息有大量重合，说明商场存在多楼层问题，并且单单依靠原始WiFi信息无法全面对楼层进行区分。 按照商场划分模型后，每个商场还是有几十上百个$label$，可以多分类，或者二分类构造多分类。（因为$label$多，二分类构造多分类会遇到$label$不均衡的问题，需依靠采样来解决） 百万数据，训练时间成本太高，需要在特征工程、反复验证时减小时间成本。（前期用$LR$模型试错） 也算带有时间序列，如何构造可靠的线下验证集 顾客数据训练集与预测集交集仅为三分之一，顾客特征是否为噪音（我们尝试过顾客特征空值填充、保留，取顾客有交集的单独训练，效果均不好） 我们的具体操作数据预处理 删除公共WiFi：如果一个WiFi在一个以上商场出现或者在某个商场覆盖率超过一定范围，那么可以断定此WiFi为噪音信息 删除移动热点或者出现次数较少的WiFi：如果一个wifi出现次数少于某个阈值或者出现时间仅有一天，那么可以断定次WiFi为噪音信息 训练集和测试集WiFi取交集，因为对于以后要构建的WiFi指纹 删除WiFi信息为空的记录 $GPS$离群点的删除 定义不同等级稳定WiFi列表（不同等级原因某些特征防止维度爆炸），WiFi出现总次数超过一定阈值或者频率前200 字典：shop历史WiFi 经纬度信息该题目给了两种经纬度信息店铺经纬度和购买发生时候的经纬度（预测集只有后者）。两种经纬度理论上差距应该很小，实际差距很大。单独利用经纬度信息训练的结果并不理想，印象中只有70%多一点。 对经纬度数值做了泛化（精确小数点位数），印象中最后的粒度可能在10米左右 $L1、L2$距离，在商场中随意选择一个点做基准，经纬度到这个点的距离 经纬度聚类、以商铺经纬为质心聚类，然后哑编码（没有提升，后来没用） 时间特征的处理 饭点指示器（万分位的提升） 早晨深夜指示器，因为这两种店铺可能比较固定 用户特征构造的所有的用户特征在我们这都是坑，一是因为训练集、预测集用户交集只有$1/3$左右，我们尝试过空值填充、空值保留、只保留交集部分用户特征、对交集部分用户单独训练预测…均是强力的反向上分特征。另一个原因可能是，每个用户在数据统计量均不足，没有统计意义，所以这就类似于$ID$类特征，妥妥没有现实意义的过拟合特征。 用户购买力 用户常去商店 用户与WiFi相关特征 WiFi特征WiFi特征是最为重要的特征，我们构建的如下的WiFi特征 当前记录连接到的最强WiFi仅用这一个特征采取规则预测准确率也可达到80%，是个强特。哑编码。 记录中稳定WiFi的数量商场WiFi原点根据商场稳定WiFi（等级频率前50）建立一个基准点，计算每条数据到基准点的距离（离散化之后），没有的稳定WiFi按照强度为-99处理。 WiFi指纹根据商场稳定WiFi（等级频数大于20），训练集、预测集上下$concat$将所有WiFi id展开为特征，值为当前记录对应WiFi的强度，将强度离散化（粒度为10）. WiFi评分（最后仅用于规则）遍历数据建立嵌套字典$WiFi_score$，一层$key$为WiFi id，二层$key$为一层WiFi出现过的店铺，二层$val$为历史上该WiFi在该店铺强度的中位数。打分，对于每条数据的稳定WiFi信息，遍历嵌套字典$WiFi_score$，用打分函数对每条WiFi出现过的店铺打分。分数结果最高的直接作为结果。打分函数如下： f=1-tanh({|power_{now}-power_{middle}|\over k})其中： $tanh={e^x-e^{-x}\over e^x+e^{-x}}$ $power_{now}$表示当前记录中当前WiFi强度 $power_{middle}$表示嵌套字典中当前WiFi对各商店的强度中位数 $k$为参数，最后确定值为2 仅仅用该种规则，不用机器学习模型，精度就可以达到87%+（还是89%+来着）。但是当时将所得结果转化为特征加入模型训练，分数强力掉了一波。不用说，这里用了训练集构造的规则结果当特征继续加入训练集训练当然会造成$label leak$，也就是强力过拟合。 遗憾的是，当时没考虑采用滑动窗口，更没有使用候选集，这个特征就放弃了…如果采用滑动窗口，在特征采取区间构造嵌套字典$WiFi_score$，打分结果加在训练区间的特征，结果应该不会差。 如果在滑动窗口的基础上，采用候选集（甚至可以用这个打分函数构造候选集），并把分数当做该条记录对候选商店的特征，绝对会是个强力特征（这里其实可以理解为类条件概率） WiFi id组合的最长公共子串典型看起来巧妙没有卵用系列 xgb构造特征组合用$model.apply()$返回叶子节点$index$构造特征。注意此部分一定要划分数据集，即一部分数据用于生成特征，另一部分数据集用于加入该部分生成特征并训练。 总结以上就是比赛用到的、或者有代表性值得拿出来说的特征。这里发现其实我们的特征并不多，其实我们比赛时候尝试构造过远远比这多得多的特征，只是因为效果或者训练时间的原因，最后筛选剩这些。挖空心思构造各种想法复杂、实现困难、强力掉分的反向特征实在是能锤炼人强大的灵魂 ：）。 滑窗的欠缺 无法构造统计类特征，构造了会造成过拟合 训练集过大时间成本过高 既然也是时间序列，信息或多或少有时效性（比如WiFi） 候选集的欠缺 无法构造联合特征，商店-WiFi，商店-经纬度等等 完全丢弃了任何商店特征，包括特别重要的统计特征 总之，这两种的欠缺也早早为我们的特征工程确定了天花板，也大大限制了我们特征的提取，少了这两项能够造的有效特征数量至少少了70% 数据集的划分这里说一下我们实际的数据集划分，在最后给出可能的改进。 这是个时间相关的问题，而且数据量比较大，就放弃了分层$k$折交叉，我们选取一个周作为本地验证集 模型训练模型选择最开始我们用的是$xgboost$的多分类模型，分商场进行预测，效果一般。然后转向使用二分类实现多分类，提升显著。具体为每次将一个$label$作为正例，其他作为反例，每次输出每条记录是该正例的概率，最后以最大概率的$label$作为预测结果 二分类&amp;样本不均衡每次有将近100个$label$每次只将一个当做正例，正负样本比会非常小，会导致时间成本过长，精度下降，需要靠采样解决。 我们直接放弃了上采样跟随机下采样，前者会加大时间成本后者因为样本比相差悬殊，随机采样可能破坏边界样本的分布。我们直接选用了用规则下采样以保存边界有效信息，具体通过购买经纬度与店铺经纬度的距离与当前样本$top2$强度任一WiFi存在历史列表的shop的并集。这样加快了训练速度，但是精度略有损失，可能因为评价指标是$AUC$所以对样本不均衡不敏感。我们最后还是放弃了采样。 二分类还会造成一个问题：每次对于一个模型训练的时候，正样本平均只有小几百，负样本可能大几千甚至上万。这样正样本太少可能造成过拟合，因此我们模型部分着重调整了泛化部分的参数。 模型参数xgb的模型参数就常用的那几个： 1、eta[默认0.3]和GBM中的 learning rate 参数类似。 通过减少每一步的权重，可以提高模型的鲁棒性。 典型值为0.01-0.2。 2、min_child_weight[默认1]建立每个模型所需要的最小样本数。决定最小叶子节点样本权重和。 和GBM的 min_child_leaf 参数类似，但不完全一样。XGBoost的这个参数是最小样本权重的和，而GBM参数是最小样本总数。 这个参数用于避免过拟合。当它的值较大时，可以避免模型学习到局部的特殊样本。 但是如果这个值过高，会导致欠拟合。这个参数需要使用CV来调整。 3、max_depth[默认6]和GBM中的参数相同，这个值为树的最大深度。 这个值也是用来避免过拟合的。max_depth越大，模型会学到更具体更局部的样本。 需要使用CV函数来进行调优。 典型值：3-10 4、max_leaf_nodes树上最大的节点或叶子的数量。 可以替代max_depth的作用。因为如果生成的是二叉树，一个深度为n的树最多生成n2个叶子。 如果定义了这个参数，GBM会忽略max_depth参数。 5、gamma[默认0]在节点分裂时，只有分裂后损失函数的值下降了，才会分裂这个节点。Gamma指定了节点分裂所需的最小损失函数下降值。 这个参数的值越大，算法越保守。这个参数的值和损失函数息息相关，所以是需要调整的。 6、max_delta_step[默认0]这参数限制每棵树权重改变的最大步长。如果这个参数的值为0，那就意味着没有约束。如果它被赋予了某个正值，那么它会让这个算法更加保守。 通常，这个参数不需要设置。但是当各类别的样本十分不平衡时，它对逻辑回归是很有帮助的。 这个参数一般用不到，但是你可以挖掘出来它更多的用处。 7、subsample[默认1]和GBM中的subsample参数一模一样。这个参数控制对于每棵树，随机采样的比例。 减小这个参数的值，算法会更加保守，避免过拟合。但是，如果这个值设置得过小，它可能会导致欠拟合。 典型值：0.5-1 8、colsample_bytree[默认1]和GBM里面的max_features参数类似。用来控制每棵随机采样的列数的占比(每一列是一个特征)。 典型值：0.5-1 9、colsample_bylevel[默认1]用来控制树的每一级的每一次分裂，对列数的采样的占比。 我个人一般不太用这个参数，因为subsample参数和colsample_bytree参数可以起到相同的作用。但是如果感兴趣，可以挖掘这个参数更多的用处。 10、lambda[默认1]权重的L2正则化项。(和Ridge regression类似)。 这个参数是用来控制XGBoost的正则化部分的。虽然大部分数据科学家很少用到这个参数，但是这个参数在减少过拟合上还是可以挖掘出更多用处的。 11、alpha[默认1]权重的L1正则化项。(和Lasso regression类似)。 可以应用在很高维度的情况下，使得算法的速度更快。 12、scale_pos_weight[默认1]在各类别样本十分不平衡时，把这个参数设定为一个正值，可以使算法更快收敛。 真正调参方法 选择较高的学习速率(learning rate)。一般情况下，学习速率的值为0.1。但是，对于不同的问题，理想的学习速率有时候会在0.05到0.3之间波动。选择对应于此学习速率的理想决策树数量。 对于给定的学习速率和决策树数量，进行决策树特定参数调优(max_depth, min_child_weight, gamma, subsample, colsample_bytree)。 xgboost的正则化参数的调优。(lambda, alpha)。这些参数可以降低模型的复杂度，从而提高模型的表现。 降低学习速率，确定理想参数。 实际上比赛的调参因为数据量巨大，样本充足，所以实际上参数不是太差的话对结果影响不是很大。况且大的数据量调参时间成本太高，而且这个比赛自始至终不是一个分类器，90+的商场每个商场有多少个$label$就有多少个分类器，总共上千个不同分类器，也不可能找出每个分类器最适合的参数。因此本次比赛只是在刚开始时调整了一下树的深度，最后的时候调整了一下过拟合的参数。 模型融合最后我们取了$0.65xgb+0.35lgb$简单的加权融合，融合方法是直接把二分类生成的概率加权相加，选出概率最大的当做$label$。 其实可以再融合上多分类，提高模型融合的多样性，但是因为我们没分滑窗，训练时间巨长，就没再添加。 本次比赛不可能$stacking$，就像不可能$CV$一样，训练一次时间都受不了，更别说反复多次。 后话这次比赛成绩不算好，但第一次正式参赛，通过比赛收获了很多，一整套流程熟悉了一遍，对模型、特征的理解也深了好多。我跟朋友两个一步一个坑，摸着石头过河，心情大起大落也是锻炼了心态…最后初赛要截止时候，我们还在复赛线边缘徘徊，眼看着自己慢慢掉出复赛线，反复讨论着各种可能，尝试构造各种复杂但看起来有意义的特征，每次连夜实现，怀着希望入睡，第二天都会被反向上分现实打脸。记得第一次掉出前100时候，我俩彻夜讨论到凌晨五点出了一套方案，从绝望到满怀希望，实际上上第二天还是究极反向上分，又坐了一次过山车：）（其实现在想想那套方案是明显的$label leak$）。 现在想想，这次比赛主要差在了套路经验上，没能构造滑窗和候选集严重限制了特征的构造，几乎所有统计特征、交叉特征、先验后延特征都无法构造，有效特征数量至少少了70%。特别是候选集，之前闻所未闻，各种地方也了解不到，后来才知道，这种方法可能从之前摩拜比赛大佬们一路传承了下来… 对样本特征的理解差也导致我们踩了好多坑。在一开始对数据分布、统计特征的构造不清楚导致我们训练时间成本巨大。现在想想当时构造的好多看起来有效的特征其实都是明显的$label leak$（先从当前数据提取特征，再用当前数据训练模型），这次比赛过后对标签泄露、过拟合是印象深刻了。]]></content>
      <categories>
        <category>比赛</category>
      </categories>
      <tags>
        <tag>大数据竞赛</tag>
        <tag>天池</tag>
        <tag>CCF</tag>
        <tag>wifi定位</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[L1、L2正则化]]></title>
    <url>%2F2018%2F01%2F01%2FL1%E3%80%81L2%E6%AD%A3%E5%88%99%E5%8C%96%2F</url>
    <content type="text"><![CDATA[从概率角度推导 本部分引知乎用户bsdelf 从概率论的角度： $Least Square$的解析解可以用$Gaussian$分布以及最大似然估计求得 $Ridge$回归可以用$Gaussian$分布和最大后验估计解释 $Lasso$回归可以用$Laplace$分布和最大后验估计解释 首先假设线性回归模型具有如下形式： f(x)=\sum^d_{j=1}{x_jw_j}+\epsilon=\mathbf{xw^T}+\epsilon其中$\mathbf{x}\in \mathbb{R}^{1\times d}，\mathbf{w}\in \mathbb{R}^{n\times d}$，误差$\epsilon\in\mathbb{R}$。 当前一直$\mathbf{X}=(\mathbf{x_1,x_2,…,x_n})^T\in\mathbb{R}^{n\times d}，\mathbf{y}\in\mathbb{R}^{n\times 1}$，怎样求$\mathbf w$呢？ 策略1：假设$\epsilon_i\sim N(0,\sigma^2)$，也就是说$\mathbf{y_i}\sim N(\mathbf{x_iw^T,\sigma^2})$，那么用最大似然估计推导： $\arg\max_{\mathbf{w}}L(\mathbf{w})$ $=\ln \prod^n_{i=1}{1\over \sigma\sqrt{2\pi}}\exp (-{1\over 2}({\mathbf y_i-\mathbf{x_iw^T}\over\sigma})^2)$ $-{1\over 2\sigma^2}\sum^n_{i=1}(\mathbf y_i-\mathbf{w_iw^T})^2-n\ln \sigma\sqrt{2\pi}$ $\arg\min_{\mathbf w}f(\mathbf w)$ $=\sum^n_{i=1}(\mathbf y_i-\mathbf{x_iw^T})^2$ $=||\mathbf y-\mathbf{Xw^T}||^2_2$ 这不就是最小二乘吗？ 策略2:假设$\epsilon_i\sim N(0,\sigma^2)$，$\mathbf w_i\sim N(0,\tau^2)$，那么最后又最大后验估计推导： $\arg\max_\mathbf{w}L(\mathbf w)$ $=\ln\prod^n_{i=1}{1\over \sigma\sqrt{2\pi}}\exp(-{1\over 2}({\mathbf y_i-\mathbf{x_iw^T}\over \sigma})^2)·\prod^d_{j=1}{1\over \tau\sqrt{2\pi}}\exp(-{1\over 2}({\mathbf w_j\over\tau})^2)$ $=-{1\over 2\sigma^2}\sum^n_{i=1}(\mathbf{y}_i-\mathbf{x_iw^T})^2-{1\over 2\tau^2}\sum^d_{j=1}\mathbf w^2_j-n\ln\sigma\sqrt{2\pi}-d\ln\tau\sqrt{2\pi}$ $\arg\min_\mathbf wf(\mathbf w)$ $=\sum^n_{i=1}(\mathbf y_i-\mathbf{x_iw^T})^2+\lambda\sum^d_{j=1}\mathbf w_j^2$ $=||\mathbf y-\mathbf{Xw^T||^2_2+\lambda||\mathbf w||^2_2}$ 这不就是$Ridge$回归吗？ 策略3：假设$\epsilon_i\sim N(0,\sigma^2)$，$\mathbf w_i\sim Laplace(0,b)$，那么最后又最大后验估计推导： $\arg\max_\mathbf{w}L(\mathbf w)$ $=\ln\prod^n_{i=1}{1\over \sigma\sqrt{2\pi}}\exp(-{1\over 2}({\mathbf y_i-\mathbf{x_iw^T}\over \sigma})^2)·\prod^d_{j=1}{1\over 2b}\exp(-{|\mathbf w_j|\over b})$ $=-{1\over 2\sigma^2}\sum^n_{i=1}(\mathbf{y}_i-\mathbf{x_iw^T})^2-{1\over 2\tau^2}\sum^d_{j=1}|\mathbf w_j|-n\ln\sigma\sqrt{2\pi}-d\ln\tau\sqrt{2\pi}$ $\arg\min_\mathbf wf(\mathbf w)$ $=\sum^n_{i=1}(\mathbf y_i-\mathbf{x_iw^T})^2+\lambda\sum^d_{j=1}|\mathbf w_j|$ $=||\mathbf y-\mathbf{Xw^T||^2_2+\lambda||\mathbf w||_1}$ 这不就是$Lasso$吗？ $L1、L2$正则化各自的特点 $L1$正则化$L1$范数是指向量中各个元素绝对值之和，也叫“稀疏规则算子”。那么为什么$L1$范数会使权值稀疏？实际上：任何规则化孙子，如果它在$w_i=0$的地方不可微，并且可以分解为一个”求和”的形式，那么这个规则化算子就可以实现稀疏 权值稀疏的好处： 1）特征选择： 没有信息的特征权值比较小。 2）可解释性： 我们可以相信权重非0的特征上面提供的信息远大于筛选掉的信息 $L1$正则化不可导： L1不可导可以使用$Proximal Algorithms$或者$ADMM$来解决。 $L2$正则化$L2$范数又称“岭回归”，也叫“权值衰减”。让权值都趋向0，可以修正过拟合，原因可能权值系数小了，模型“多项式性”就不明显了？。 1）学习理论角度： $L2$范数可以防止过拟合，提升模型泛化能力 2）优化计算角度： $L2$有助于处理$condition number$不好的情况 这篇知乎文章对$L2$正则化从四个方面深度剖析。 $mse$损失函数 L(w)={1\over N}(y-\hat y)^T(y-\hat y)={1\over N}(y-Xw)^T(y-Xw)为使其达到最小，则关于$w$的偏导为0： \hat w=(X^TX)^{-1}X^Ty然而会存在$X^TX$不可逆的情况。为了避免这个问题，将$X^TX$矩阵的对角元素增大一个量$\lambda$，变成$X^TX+\lambda I$。相当于在矩阵$X^TX$中的对角线上加一个值增高他的山岭。山岭增高后矩阵变成可逆的。带入得： \hat w_{bridge}=(X^TX+\lambda I)^{-1}X^Ty 从贝叶斯的角度，假设$w$服从$N(0,\sigma^2I_p)$分布，在最大似然的基础上加入了$w$的先验知识，相当于极大化后验概率。求解该极大化的后验概率，对应的先验知识就是正则化部分 $L2$正则化的解相当于先对数据做了主成分分析，然后对于方差小的主成分方向进行惩罚。主成分分析原样保留大方差方向，去掉小方差方向，相当于一个硬选择。而$ L2$ 正则根据方差的大小施加不同程度的惩罚，相当于软选择。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>L1</tag>
        <tag>L2</tag>
        <tag>正则化</tag>
        <tag>先验概率</tag>
        <tag>后验概率</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[几种特征选择的方法]]></title>
    <url>%2F2017%2F12%2F23%2F%E5%87%A0%E7%A7%8D%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9%E7%9A%84%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[当数据预处理完成后，我们需要选择有意义的特征输入机器学习的算法和模型进行训练。通常来说，从两个方面考虑来选择特征： 特征是否发散：如果一个特征不发散，例如方差接近于0，也就是说样本在这个特征上基本上没有差异，这个特征对于样本的区分并没有什么用。 特征与目标的相关性：这点比较显见，与目标相关性高的特征，应当优选选择。除方差法外，本文介绍的其他方法均从相关性考虑。 根据特征选择的形式又可以将特征选择方法分为3种： Filter：过滤法，按照发散性或者相关性对各个特征进行评分，设定阈值或者待选择阈值的个数，选择特征。 Wrapper：包装法，根据目标函数（通常是预测效果评分），每次选择若干特征，或者排除若干特征。 Embedded：嵌入法，先使用某些机器学习的算法和模型进行训练，得到各个特征的权值系数，根据系数从大到小选择特征。类似于Filter方法，但是是通过训练来确定特征的优劣。 我们使用sklearn中的feature_selection库来进行特征选择。 $Filter$ 优点：快速，主需要统计知识 缺点：难以挖局特征之间的组合效应 方差选择法使用方差选择法，先要计算各个特征的方差，然后根据阈值，选择方差大于阈值的特征。 相关系数法使用相关系数法，先要计算各个特征对目标值的相关系数以及相关系数的P值。 卡方检验经典的卡方检验是检验定性自变量对定性因变量的相关性。假设自变量有N种取值，因变量有M种取值，考虑自变量等于i且因变量等于j的样本频数的观察值与期望的差距，构建统计量： \chi^2=\sum{(A-E)\over E}其中$A$为实际值，$E$为理论值。$\chi^2$用于衡量两个变量实际值与联合分布理论值的差异程度（也就是卡方检验的核心思想），包含了一下两个信息： 实际值与理论值偏差的绝对大小 差异程度与理论值的相对大小 这个统计量含义就是自变量对因变量的相关性。 可以这么理解，对于这两个特征的联合分布矩阵，这两个变量不相关，那么通过边缘概率求得的期望值（$E$）就应该越接近于真实值（$A$），$\chi^2$越小；相反，这两个变量越相关，通关边缘概率分布求得的期望值（$E$）跟真实值（$A$）的差距就应该越大，$\chi^2$越大。 因此$\chi^2$越大，说明自变量与因变量相关性越强 互信息法不多说 $Wrapper$ 优点：直接面向算法优化，不需要太多知识 缺点：庞大的搜索空间，需要定义启发式策略 递归消除特征法使用一个基模型来进行多轮训练，每轮训练后，消除若干权值系数的特征，再基于新的特征集进行下一轮训练。 $Embedded$ 优点：快速，并且面向算法 缺点：需要调整结构和参数配置 基于惩罚项的特征选择法使用带$L1$惩罚项的基模型，除了筛选出特征外，同时也进行了降维。 $Lasso$的参数$\lambda$越大，参数的解越稀疏，选出的特征越少。 L1惩罚项降维的原理在于保留多个对目标值具有同等相关性的特征中的一个，所以没选到的特征不代表不重要。故，可结合L2惩罚项来优化。具体操作为：若一个特征在L1中的权值为1，选择在L2中权值差别不大且在L1中权值为0的特征构成同类集合，将这一集合中的特征平分L1中的权值. 基于树模型的特征选择法各种树模型可以输出特征的重要性]]></content>
      <tags>
        <tag>特征选择</tag>
        <tag>特征工程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[剑指offer]]></title>
    <url>%2F2017%2F10%2F27%2F%E5%89%91%E6%8C%87offer%2F</url>
    <content type="text"><![CDATA[剑指offer二维数组中的查找Q:在一个二维数组中，每一行都按照从左到右递增的顺序排序，每一列都按照从上到下递增的顺序排序。请完成一个函数，输入这样的一个二维数组和一个整数，判断数组中是否含有该整数。 思路：思路从右上开始，右上为一行最大，一列最小。target&lt;右上，删一列; target&gt;右上,删一行 Code 1234567891011121314class Solution1: # array 二维列表 def Find(self, target, array): # write code here i = 0 j = len(array[0])-1 while i&lt;len(array) and j&gt;=0: if array[i][j] == target: return True elif array[i][j] &gt; target: j -= 1 else: i += 1 return False 替换空格Q:请实现一个函数，将一个字符串中的空格替换成“%20”。例如，当字符串为We Are Happy.则经过替换之后的字符串为We%20Are%20Happy。 思路：python式。。 其他语言思路：从头到尾遍历数空格数量，然后从后往前遍历，碰到空格后根据之前数好的数量移位 Code 123456class Solution2: # s 源字符串 def replaceSpace(self, s): # write code here s= s.replace(' ', '%20') return s 从尾到头打印链表Q:输入一个链表，从尾到头打印链表每个节点的值 思路：遍历链表每次遍历的val从头插入list Code 12345678910class Solution3: # 返回从尾部到头部的列表值序列，例如[1,2,3] def printListFromTailToHead(self, listNode): # write code here list = [] p = listNode while p: list = [p.val] + list p = p.next return list 重建二叉树Q:输入某二叉树的前序遍历和中序遍历的结果，请重建出该二叉树。假设输入的前序遍历和中序遍历的结果中都不含重复的数字。例如输入前序遍历序列{1,2,4,7,3,5,6,8}和中序遍历序列{4,7,2,1,5,3,8,6}，则重建二叉树并返回。 思路：先序第一个为根节点，中序中根节点左边全在左子树，右边全在右子树。据此用递归建树 Code 123456789101112131415161718192021222324252627# 我的解法# 对len==1的list判定，繁琐class Solution32: # 返回构造的TreeNode根节点 def reConstructBinaryTree(self, pre, tin): # write code here root = TreeNode(pre[0]) if len(pre) == 1 and len(tin) == 1: return root root_index = tin.index(pre[0]) if root_index != 0: root.left = self.reConstructBinaryTree(pre[1:root_index+1],tin[:root_index]) if root_index != len(tin)-1: root.right = self.reConstructBinaryTree(pre[root_index+1:],tin[root_index+1:]) return root# 精简解法# 对空list的判定，鲁棒性更强，更精简def reConstructBinaryTree(self, pre, tin): # write code here if not pre or not tin: return None root = TreeNode(pre[0]) index = tin.index(root.val) root.left = self.reConstructBinaryTree(pre[1:index+1], tin[0:index]) root.right = self.reConstructBinaryTree(pre[index+1:], tin[index+1:]) return root 旋转数组的最小数字Q:把一个数组最开始的若干个元素搬到数组的末尾，我们称之为数组的旋转。 输入一个非递减排序的数组的一个旋转，输出旋转数组的最小元素。 例如数组{3,4,5,1,2}为{1,2,3,4,5}的一个旋转，该数组的最小值为1。 NOTE：给出的所有元素都大于0，若数组大小为0，请返回0。 思路：太简单，不说了 Code 12345678910111213class Solution5: def minNumberInRotateArray(self, rotateArray): # write code here nums = rotateArray if not nums: return 0 p = len(nums) - 1 min = nums[0] while p &gt; 0 and nums[p-1]&lt;= nums[p]: p -= 1 if nums[p] &lt; min: min = nums[p] return min 斐波那契数列Q:大家都知道斐波那契数列，现在要求输入一个整数n，请你输出斐波那契数列的第n项。n&lt;=39 思路：分别用while 与 递归实现 Code 1234567891011121314151617181920212223242526# 不用递归，避免重复运算，运行时间短class Solution6: def Fibonacci(self, n): # write code here if n == 0: return 0 if n &lt; 3: return 1 pre = 1 post = 1 count = 3 while count &lt;= n: temp = post post = pre + post pre = temp count += 1 return post# 递归，代码简单，运行超时class Solution62: def Fibonacci(self, n): # write code here if n == 0: return 0 if n == 1: return 1 return self.Fibonacci(n-1) + self.Fibonacci(n-2) 跳台阶Q:一只青蛙一次可以跳上1级台阶，也可以跳上2级。求该青蛙跳上一个n级的台阶总共有多少种跳法。 思路：斐波那契数列问题，斐波那契数列前移一位 Code 1234567891011121314class Solution7: def jumpFloor(self, number): # write code here if number &lt; 2: return 1 pre = 1 post = 1 count = 2 while count &lt;= number: temp = post post = pre + post pre = temp count += 1 return post 变态跳台阶Q:一只青蛙一次可以跳上1级台阶，也可以跳上2级……它也可以跳上n级。求该青蛙跳上一个n级的台阶总共有多少种跳法 思路： F(n) = F(n-1)+F(n-2)+F(n-3)+….+F(1) 第n项是前n-1项的和，所以第n+1项是第n项的两倍，都是二的指数次方 Code 1234class Solution8: def jumpFloorII(self, number): # write code here return 2 ** (number-1) 矩形覆盖Q:我们可以用21的小矩形横着或者竖着去覆盖更大的矩形。请问用n个21的小矩形无重叠地覆盖一个2*n的大矩形，总共有多少种方法？ 思路：还是个斐波那契问题 n*2 矩形覆盖时候最后一步要不覆盖田，要不覆盖日，因此是斐波那契问题F(n) = F(n-1)+F(n-2)只不过题目规定F(0)=0 Code 12345678910111213141516class Solution9: def rectCover(self, number): # write code here if not number: return 0 if number &lt; 2: return 1 pre = 1 post = 1 count = 2 while count &lt;= number: temp = post post = pre + post pre = temp count += 1 return post 二进制中1的个数Q:输入一个整数，输出该数二进制表示中1的个数。其中负数用补码表示。 思路1：一个整数减去1，再和原整数做 与 运算，会把该整数最右边的1变成0（用补码表示的负数也是）。所以一个整数中有多少个1就可以进行多少次这种运算该法用python写超时 Code 12345678class Solution10: def NumberOf1(self, n): # write code here i = 0 while n: n = n &amp; (n-1) i += 1 return i 思路2：因为右移数字n会造成死循环（负数第一位是1），可以先把n和1做与运算，判断n的最低位是不是1，接着把1左移一位得到2，再和n做与运算……反复左移就能判断n的其中一位是不是1 注意：1要是无符号整数(unsigned int) Code 123456789101112int NumberOf1(int n)&#123; int count = 0; unsigned int flag = 1; while(flag) &#123; if(n&amp;flag) count ++ flag = flag &lt;&lt; 1 &#125; return count;&#125; 数值的整数次方Q:给定一个double类型的浮点数base和int类型的整数exponent。求base的exponent次方。 思路：Life is short, you need python Code 1234class Solution11: def Power(self, base, exponent): # write code here return pow(base, exponent) 调整数组是奇数位于偶数前并保证奇数偶数各自相对位置Q:输入一个整数数组，实现一个函数来调整该数组中数字的顺序，使得所有的奇数位于数组的前半部分，所有的偶数位于位于数组的后半部分，并保证奇数和奇数，偶数和偶数之间的相对位置不变。 思路：一个list存奇数一个list存偶数，两个合起来 Code 1234567891011class Solution12: def reOrderArray(self, array): # write code here res1 = [] res2 = [] for i in range(len(array)): if array[i] % 2 == 0 : res2.append(array[i]) else: res1.append(array[i]) return res1+res2 链表中倒数第K个节点Q:请实现一个函数，将一个字符串中的空格替换成“%20”。例如，当字符串为We Are Happy.则经过替换之后的字符串为We%20Are%20Happy。 思路：构造长度为K的滑窗（构造时考虑K过大问题），将滑窗滑至最后即可 Code 12345678910111213141516171819class Solution13: def FindKthToTail(self, head, k): # write code here if k &lt; 1: return None if not head: return None post = pre = head count = k while k-1 &gt; 0: if post.next: post = post.next k -= 1 else: return None while post.next: post = post.next pre = pre.next return pre 翻转链表Q:输入一个链表，反转链表后，输出链表的所有元素。 思路：比较简单临近双指针遍历至最后，注意最后对边界值的处理 Code 123456789101112131415161718class Solution14: # 返回ListNode def ReverseList(self, pHead): # write code here if not pHead or not pHead.next: return pHead post = pHead.next pHead.next = None while post: if post.next: temp = post.next else: post.next = pHead break post.next = pHead pHead = post post = temp return post 合并两个排序链表Q:输入两个单调递增的链表，输出两个链表合成后的链表，当然我们需要合成后的链表满足单调不减规则。 思路：每次选出两个链表较小的头结点，然后用剩下的递归链接 Code 123456789101112class Solution15: # 返回合并后列表 def Merge(self, pHead1, pHead2): # write code here if not pHead1 or not pHead2: return pHead1 or pHead2 if pHead1.val &lt; pHead2.val: pHead1.next = self.Merge(pHead1.next, pHead2) return pHead1 else: pHead2.next = self.Merge(pHead1, pHead2.next) return pHead2 树的子结构Q:输入两棵二叉树A，B，判断B是不是A的子结构。（ps：我们约定空树不是任意一个树的子结构） 思路1：递归的查找子结构 注意： ①要全面遍历所有所可能（类似机器人路径中的找出所有起始位置），不能找到一对根节点相等的点，后面不等就return False； 这里体现在相等时候也留一手 or 类似与不等继续进行判断 ②因为题中给出空节点不是子结构，而我迭代又要用到对pRoot2是否为空的判断，因此，先对pRoot2判断，再单独写了个函数 ③本题求的是子结构而非子树 Code 1234567891011121314151617class Solution16: def HasSubtree(self, pRoot1, pRoot2): # write code here if not pRoot2: return False def Search(pRoot1,pRoot2): if not pRoot2: return True if not pRoot1: return False if pRoot1.val ==pRoot2.val: # 注意or的运用，遍历所有可能的节点 return True and ( (Search(pRoot1.left,pRoot2.left) and Search(pRoot1.right,pRoot2.right)) or Search(pRoot1.left,pRoot2) or Search(pRoot1.right,pRoot2) ) else: return Search(pRoot1.left,pRoot2) or Search(pRoot1.right,pRoot2) return Search(pRoot1,pRoot2) 思路2：更精简的分治递归 写一个函数判断两树是同一根节点的情况下pRoot2是否是pRoot1的子结构 在主函数里递归调用这个函数 Code 123456789101112class Solution: def HasSubtree(self, pRoot1, pRoot2): # write code here if not pRoot1 or not pRoot2: return False return self.IsSub(pRoot1,pRoot2) or self.IsSub(pRoot1.left,pRoot2) or self.IsSub(pRoot1.right,pRoot2) def IsSub(self,p1,p2): if not p2: return True if not p1 or p1.val != p2.val: return False return self.IsSub(p1.left,p2.left) and self.IsSub(p1.right,p2.right) 二叉树的镜像Q:操作给定的二叉树，将其变换为源二叉树的镜像。 思路：递归实现没什么难度 Code 1234567891011121314class Solution17: # 返回镜像树的根节点 def Mirror(self, root): # write code here def Trans(root): if not root: return temp = root.left root.left = root.right root.right = temp Trans(root.left) Trans(root.right) Trans(root) return root 顺时针打印矩阵Q:输入一个矩阵，按照从外向里以顺时针的顺序依次打印出每一个数字，例如，如果输入如下矩阵： 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 则依次打印出数字1,2,3,4,8,12,16,15,14,13,9,5,6,7,11,10. 思路：通过递归函数实现，每次添加最外一圈 注意：对行列为2的判断（里面的if判断），过程中一个不满足就代表添加完毕 Code 1234567891011121314151617181920212223242526272829303132class Solution18: # matrix类型为二维列表，需要返回列表 def __init__(self): self.List = [] def printMatrix(self, matrix): # write code here row_u = 0 row_d = len(matrix) - 1 col_l = 0 col_r = len(matrix[0]) - 1 def Printnum(matrix,row_u,row_d,col_l,col_r): if row_u &gt; row_d or col_l &gt; col_r: return if row_u &lt;= row_d and col_l &lt;= col_r: for j in range(col_l,col_r+1): self.List.append(matrix[row_u][j]) row_u += 1 if row_u &lt;= row_d and col_l &lt;= col_r: for i in range(row_u,row_d+1): self.List.append(matrix[i][col_r]) col_r -= 1 if row_u &lt;= row_d and col_l &lt;= col_r: for j in range(col_r,col_l-1,-1): self.List.append(matrix[row_d][j]) row_d -= 1 if row_u &lt;= row_d and col_l &lt;= col_r: for i in range(row_d,row_u-1,-1): self.List.append(matrix[i][col_l]) col_l += 1 Printnum(matrix,row_u,row_d,col_l,col_r) Printnum(matrix,row_u,row_d,col_l,col_r) return self.List 包含Min函数的栈Q:定义栈的数据结构，请在该类型中实现一个能够得到栈最小元素的min函数。 思路：python式。。 Code 123456789101112131415class Solution19: def __init__(self): self.stack = [] def push(self, node): # write code here self.stack.append(node) def pop(self): # write code here self.stack = self.stack[:-1] def top(self): # write code here return self.stack[-1] def min(self): # write code here return min(self.stack) 栈的压入、弹出序列Q:输入两个整数序列，第一个序列表示栈的压入顺序，请判断第二个序列是否为该栈的弹出顺序。假设压入栈的所有数字均不相等。例如序列1,2,3,4,5是某栈的压入顺序，序列4，5,3,2,1是该压栈序列对应的一个弹出序列，但4,3,5,1,2就不可能是该压栈序列的弹出序列。（注意：这两个序列的长度是相等的） 思路：弹出一个便从压入list中记录index为p，并删除 这个元素。下一个弹出的元素的index只能是p的前一个位置(pre)或者后面位置，弹出后便继续更新p 注意：压入list中删除元素后要更新index。list.remove(val)删除后会更新index，正好符合这道题。 Code 1234567891011121314151617class Solution20: def IsPopOrder(self, pushV, popV): # write code here pre = p = -1 for i in range(len(popV)): # 判断pushV popV是否一致 if popV[i] not in pushV: return False if pushV.index(popV[i]) &lt; pre: return False p = pushV.index(popV[i]) pushV.remove(popV[i]) if p == 0: pre = -1 else: pre = p - 1 return True 从上往下打印二叉树Q:从上往下打印出二叉树的每个节点，同层节点从左至右打印。 思路：简单的层序遍历 注意：node=[] res=[] 跟 node=res=[]的区别，前面两个是两个[]，后面两个是同一个[] Code 1234567891011121314151617class Solution21: # 返回从上到下每个节点值列表，例：[1,2,3] def PrintFromTopToBottom(self, root): # write code here if not root : return [] node = [] res = [] node.append(root) while node: res.append(node[0].val) if node[0].left: node.append(node[0].left) if node[0].right: node.append(node[0].right) node = node[1:] return res 二叉树的后序遍历序列Q:输入一个整数数组，判断该数组是不是某二叉搜索树的后序遍历的结果。如果是则输出Yes,否则输出No。假设输入的数组的任意两个数字都互不相同。 思路：二叉搜索树的中序遍历序列便相当于升序序列，因此将原序列排序便得到中序序列。然后判断能否成树判断能否成树的方法：后序最末位为根节点，通过此节点在中序中的位置（mid_index）分割左右子树节点。 递归判断不能成树的条件是：递归中一旦出现两序列节点组成不一致的情况（最重要的思想！！！想死我了） 注意：其实此题不需要用到二叉搜索树left&lt;root&lt;right，或者说这个条件的全部信息都转化为升序序列为中序序列刚看此题想到中序序列以为此题得到解决，但是拘泥于二叉搜索树的性质写不出递归，其实可完全转化为一个中序一个后序能否成树问题。能否成树又可通过递归过程序列组成是否全程一致来判断。 Code 12345678910111213141516class Solution22: def VerifySquenceOfBST(self, sequence): # write code here if not sequence: return False mid = sorted(sequence) post = sequence def Define(mid, post): if not mid and not post: return True # 不能成树的判定 if set(mid)!= set(post): return False mid_index = mid.index(post[-1]) return True and Define(mid[:mid_index],post[:mid_index]) and Define(mid[mid_index+1:],post[mid_index:-1]) return Define(mid,post) 二叉树中和为某一值的路径Q:输入一颗二叉树和一个整数，打印出二叉树中结点值的和为输入整数的所有路径。路径定义为从树的根结点开始往下一直到叶结点所经过的结点形成一条路径。 思路：定义全局list：self.res，用rest记录剩下路径所需拟合值，PathList记录当前路径。递归遍历节点如果root.val拟合完毕，将PathList并入全局res，并且rest重置为expectNumber，重置PathList=[]，从孩子节点从头递归；如果root.val不拟合rest，则将这个节点加入当前PathList，左右孩子传承此PathList继续递归寻找接下来的Path，左右孩子也要不继承PathList把rest重置为expectNumber从头递归 注意： ①root.val拟合或者不拟合都要在左右孩子加一个递归重置rest为expectNumber，重置PathList=[]继续寻找 ②左右节点继承当前PathList递归时，PathList要分开指代，要不右节点会继承左节点的PathList ③PathList2=PathList这种方法指代，这俩指针还是指向同一list Code 123456789101112131415161718192021222324252627282930313233class Solution23: # 返回二维列表，内部每个列表表示找到的路径 def __init__(self): self.res = [] def FindPath(self, root, expectNumber): # write code here def Path(root, expectNumber, rest, PathList=[] ): if not root: return if root.val == rest: PathList=PathList+[root.val] # 不加以下这个if结构是任意子路径，加了代表路径结尾必须是叶子节点。 if not root.left and not root.right: self.res.append(PathList) Path(root.left,expectNumber,expectNumber,[]) Path(root.right,expectNumber,expectNumber,[]) return else: PathList.append(root.val) rest -= root.val # 将此节点填入路径继续递归 # 接下来两个递归会操作同一个PathList，这两个不应该操作统一list # 直接令PathList2=PathList这俩还是指向同一个 PathList2 = [] PathList2 += PathList Path(root.left,expectNumber,rest,PathList) Path(root.right,expectNumber,rest,PathList2) # 从此节点的子节点开始从头寻找路径 Path(root.left,expectNumber,expectNumber,[]) Path(root.right,expectNumber,expectNumber,[]) return Path(root,expectNumber,expectNumber,[]) return self.res 复杂链表的复制Q:输入一个复杂链表（每个节点中有节点值，以及两个指针，一个指向下一个节点，另一个特殊指针指向任意一个节点），返回结果为复制后复杂链表的head。（注意，输出结果中请不要返回参数中的节点引用，否则判题程序会直接返回空） 思路1：难点在给随机指针赋值时候怎样指向已存在的节点，这里的思路是通过next复制节点时，构造原节点与新节点一一对应的字典此方法因为用到哈希，空间复杂度较大 Code 123456789101112131415161718192021222324252627class Solution24: # 返回 RandomListNode def Clone(self, pHead): # write code here if not pHead: return None Node_dic = &#123;&#125; pre = pHead clone_head = RandomListNode(pre.label) p = clone_head Node_dic[pre] = p while pre: pre = pre.next if pre : temp = RandomListNode(pre.label) p.next = temp p=p.next # 边复制节点边构造哈希表 Node_dic[pre] = p pre = pHead p = clone_head while pre: if pre.random: p.random = Node_dic[pre.random] pre = pre.next p = p.next return clone_head 思路2：省去了空间复杂度，复制链表时一一间隔，将新节点插入原链表 注意：不能用一个while（调整random指针的while）直接将合并的链表拆开 因为如果有random指向前面会出错，因为已经不是一一间隔了 Code 1234567891011121314151617181920212223242526272829303132333435class Solution24_2: # 返回 RandomListNode def Clone(self, pHead): # write code here if not pHead: return pHead p = pHead # 将新节点一一间隔插入原链表 while p: temp = RandomListNode(p.label) temp.next = p.next p.next = temp p = p.next.next pre = pHead post = pre.next new_head = pre.next while pre: if pre.random: post.random = pre.random.next pre = post.next if not pre: continue if pre.next: post = pre.next pre = pHead post = pre.next # 不能直接在上一个while直接将两链表分开，如果有random指向前面会出错，因为已经不是一一间隔了 while pre: pre.next = post.next # 考虑到尾节点情况 if post.next: post.next = post.next.next pre = pre.next post = post.next return new_head 二叉搜索树与双向链表Q:输入一棵二叉搜索树，将该二叉搜索树转换成一个排序的双向链表。要求不能创建任何新的结点，只能调整树中结点指针的指向。 思路：二叉搜索树中序序列便是升序序列，用中序遍历构造链表 注意：需要两个全局变量记录起点指针self.head，还有上一节点的指针self.p Code 12345678910111213141516171819202122232425class Solution25: def __init__(self): self.p = None self.head = None def Convert(self, pRootOfTree): # write code here root = pRootOfTree def Trans(root): if not root: return #temp保留右指针 temp_left = root.left temp_right = root.right self.Convert(temp_left) if not self.p: self.p = root self.head = root else: self.p.right = root root.left = self.p self.p = root self.Convert(temp_right) return Trans(root) return self.head 字符串的排列Q:输入一个字符串,按字典序打印出该字符串中字符的所有排列。例如输入字符串abc,则打印出由字符a,b,c所能排列出来的所有字符串abc,acb,bac,bca,cab和cba。 思路1：先用递归将所有可能的情况列出来，用sorted（）解决字典排序问题列出所有情况的方法：分成两个列表s1,s2 ，每次s2中的一个元素加入s1当做下一个s1，其余元素当做下一个s2，进行递归自己的解法：非常不优雅，先将str转为list，在转为str添加进去，还用到了py自带的排序解决字典序问题 如果不用自带排序，则可以参考LeetCode31 注意：因为python中的字符串不可以更改，所以，先将str转为list，在转为str添加进去 Code 1234567891011121314151617181920212223class Solution26_1: def __init__(self): self.res = [] def Permutation(self, ss): # write code here if not ss: return [] def allkinds(s1,s2): if len(s2) == 1: s = ''.join(s1+s2) self.res.append(s) return for i in range(len(s2)): # 将选中元素替换到s2第一位 temp = s2[i] s2[i]=s2[0] s2[0]=temp allkinds(s1+[s2[0]],s2[1:]) return s = sorted(ss) allkinds([],s) self.res = list(set(self.res)) return sorted(self.res) 思路2：itertools.permutations用来返回所有排列（元组形式）的list’’.join() 将list、元组内的字符结合成字符串map函数将f应用于右边可迭代的每一个对象 Code 12345678import itertoolsclass Solution26_2: def Permutation(self, ss): # write code here if not ss : return [] else: return sorted(list(set(map(''.join,itertools.permutations(ss))))) 最小的k个数Q:输入n个整数，找出其中最小的K个数。例如输入4,5,1,6,2,7,3,8这8个数字，则最小的4个数字是1,2,3,4,。 思路1：python式 Code 123456class Solution28_1: def GetLeastNumbers_Solution(self, tinput, k): # write code here if k&gt;len(tinput): return [] return sorted(tinput)[:k] 思路2：partition函数法（快排）：如果下标刚好是k(k-1)则左边(k-1则包含本身)便是所求，如果下标&gt;k则递归左边，下标&lt;k递归右边 Code 1234567891011121314151617181920212223242526272829class Solution28_2: def GetLeastNumbers_Solution(self, tinput, k): # write code here if k &gt; len(tinput): return [] def Partition(nums, k): i = 0 j = len(nums) - 1 temp = nums[0] while i &lt; j: while nums[j] &gt; temp: j -= 1 nums[i] = nums[j] i += 1 while nums[i] &lt;= temp: i += 1 nums[j] = nums[i] j -= 1 nums[i] = temp if i == k - 1 or i == k - 2: return elif i &gt; k - 1: return Partition(nums[:i], k) else: return Partition(nums[i + 1:], k - i - 1) Partition(tinput, k) return sorted(tinput[:k]) 连续子数组的最大和Q:在古老的一维模式识别中,常常需要计算连续子向量的最大和,当向量全为正数的时候,问题很好解决。但是,如果向量中包含负数,是否应该包含某个负数,并期望旁边的正数会弥补它呢？例如:{6,-3,-2,7,-15,1,2,2},连续子向量的最大和为8(从第0个开始,到第3个为止)。 思路： 两个变量分别记录当前累加值(self.temp)与最大累加值(self.Max)。从头到尾累加数字如果self.temp&lt;0则在每次累加之前将其置0。最关键一点就是用self.temp记录最大值 动态规划思想：用f(i)表示以第i个数字结尾的子数组的最大和则max（f(i)）可由一下迭代公式求f(i)=Datai&lt;=0) f(i)=f(i-1)+Datai&gt;0) Code 12345678910111213class Solution29: def __init__(self): self.temp = -99 self.Max = -99 def FindGreatestSumOfSubArray(self, array): # write code here for i in range(len(array)): if self.temp &lt; 0: self.temp = 0 self.temp += array[i] if self.temp &gt; self.Max: self.Max = self.temp return self.Max 整数中1出现的次数Q:求出1-13的整数中1出现的次数,并算出100-1300的整数中1出现的次数？为此他特别数了一下1~13中包含1的数字有1、10、11、12、13因此共出现6次,但是对于后面问题他就没辙了。ACMer希望你们帮帮他,并把问题更加普遍化,可以很快的求出任意非负整数区间中1出现的次数。 思路： 10个数里面有1个个位1；每100个数里面有10个十位1（10、11、12……19）；每1000个数里面有100个百位1 …… 另外需要判断是否应该考虑剩余部分，剩余部分需要判断如115 里面有 10 + (115-110+1) 个十位1 Code 1234567891011121314151617181920212223242526class Solution30: def NumberOf1Between1AndN_Solution(self, n): # write code here if n &lt; 1: return 0 if n&lt;10: return 1 result = 0 # count代表n的位数 count = len(str(n)) # 这里相当于两位到 count-1位 第count 位要单独讨论 for i in range(1,count+1): # count为完整的倍数 times = n//(10**i) # num 为每一份对应的1的个数 num = 10 ** (i-1) # rest 为除掉完整的份数剩下的 rest = n - times*(10**i) result += times * num if rest &gt;= 2 * (10**(i-1)): result += 10**(i-1) elif rest &lt; (10**(i-1)): result += 0 else: result += rest - (10**(i-1)) + 1 return result 将数组排成最小的数Q:输入一个正整数数组，把数组里所有数字拼接起来排成一个数，打印能拼接出的所有数字中最小的一个。例如输入数组{3，32，321}，则打印出这三个数字能排成的最小数字为321323。 思路：全部转化为字符串，用itertools.permutations列出全组合，直接用min 找全组合中最小的 注意：对itertools.permutations、map()、’ ‘.join的应用 Code 123456789class Solution31: def PrintMinNumber(self, numbers): # write code here if not numbers: return '' for i in range(len(numbers)): numbers[i] = str(numbers[i]) List = map(''.join,list(itertools.permutations(numbers,len(numbers)))) return int(min(List)) 序列化二叉树Q:请实现两个函数，分别用来序列化和反序列化二叉树 思路1：非递归，用栈 序列化：先序遍历得到字符串 反序列化：用栈记录字符串的节点，非#时入栈，flag指向添加方式1为左0为右，flag= =0时出栈，新元素入栈时flag = =1 遇到#号时flag= =0 注意：链接节点时候一定要注意指针。要用原节点去链接，而不是构造与原节点相同的节点链接（见注释） Code 12345678910111213141516171819202122232425262728293031323334353637383940414243class Solution_7_mine: def Serialize(self, root): # write code here if not root: return "#" else: return str(root.val)+','+self.Serialize(root.left)+','+self.Serialize(root.right) def Deserialize(self, s): # write code here s = s.split(',') if not s: return None if len(s) == 1: return TreeNode(s[0]) root = TreeNode(s[0]) stack = [root] i = 1 flag = 1 # flag==1指示left添加且不用退栈，flag==0指示右边添加且要退栈 while i&lt;len(s): if s[i] != '#': if flag == 1: stack[-1].left = TreeNode(s[i]) # stack.append(TreeNode(s[i])) 是错误的！！！这不是链接原节点，而是构造与原节点相同的节点链接 stack.append(stack[-1].left) i += 1 else: stack[-1].right = TreeNode(s[i]) pop_node = stack[-1] stack = stack[:-1] # stack.append(TreeNode(s[i])) 是错误的！！！这不是链接原节点，而是构造与原节点相同的节点链接 stack.append(pop_node.right) i += 1 flag = 1 else: if flag == 1: #stack[-1].left = None flag = 0 i += 1 else: #stack[-1].right = None stack = stack[:-1] i += 1 return root 思路2：精简的递归思路 ①通过 def init(self):构造flag全局变量指示遍历位置 ②通过self.flag指示list位置建树，建树过程类似遍历过程，只不过遇到#return ③注意l元素本身为字符，给节点赋值时注意int化 123456789101112131415161718192021class Solution: def __init__(self): self.flag = -1 def Serialize(self, root): # write code her if not root: return '#' return str(root.val)+','+self.Serialize(root.left)+','+self.Serialize(root.right) def Deserialize(self, s): # write code here self.flag += 1 l = s.split(',') if self.flag &gt;= len(l): return None if l[self.flag] == '#': return None else: root = TreeNode(int(l[self.flag])) root.left = self.Deserialize(s) root.right = self.Deserialize(s) return root 二叉树的第K个节点Q:给定一颗二叉搜索树，请找出其中的第k大的结点。例如， 5 / 3 7 / / 2 4 6 8 中，按结点数值大小顺序第三个结点的值为4。 思路：二叉搜索树中序遍历就是升序 Code 123456789101112131415class Solution_5: # 返回对应节点TreeNode def KthNode(self, pRoot, k): # write code here if k == 0 or not pRoot: return res = [] def dfs(root,res=[]): if not root: return dfs(root.left,res) res.append(root) dfs(root.right,res) dfs(pRoot,res) return res[k-1] if k&lt;=len(res) else None 矩阵中的路径Q:请设计一个函数，用来判断在一个矩阵中是否存在一条包含某字符串所有字符的路径。路径可以从矩阵中的任意一个格子开始，每一步可以在矩阵中向左，向右，向上，向下移动一个格子。如果一条路径经过了矩阵中的某一个格子，则该路径不能再进入该格子。 例如 a b c e s f c s a d e e 矩阵中包含一条字符串”bcced”的路径，但是矩阵中不包含”abcb”路径，因为字符串的第一个字符b占据了矩阵中的第一行第二个格子之后，路径不能再次进入该格子。 思路：先用嵌套for循环找出所有起始位置，对每个位置依次用递归函数搜寻 注意：每次开始要用新的空list记录路径 Code 12345678910111213141516171819202122232425262728class Solution_2: def hasPath(self, matrix, rows, cols, path): # write code here ok = [] for i in range(rows): ok.append([]) for _ in range(len(matrix)): ok[_//cols].append(matrix[_]) init = [] for i in range(rows): for j in range(cols): if ok[i][j] == path[0]: init.append([i,j]) def search(i,j,rows,cols,ok,path,List=[]): if not path: return True if i&lt;0 or j&lt;0 or i&gt;=rows or j&gt;=cols or [i,j] in List : return False if ok[i][j] == path[0]: List.append([i,j]) return True and (search(i+1,j,rows,cols,ok,path[1:],List) or search(i,j+1,rows,cols,ok,path[1:],List) or search(i-1,j,rows,cols,ok,path[1:],List) or search(i,j-1,rows,cols,ok,path[1:],List)) else: return False for x in range(len(init)): if search(init[x][0],init[x][1],rows, cols, ok, path,[]): return True return False 机器人运动范围Q:地上有一个m行和n列的方格。一个机器人从坐标0,0的格子开始移动，每一次只能向左，右，上，下四个方向移动一格，但是不能进入行坐标和列坐标的数位之和大于k的格子。 例如，当k为18时，机器人能够进入方格（35,37），因为3+5+3+7 = 18。但是，它不能进入方格（35,38），因为3+5+3+8 = 19。请问该机器人能够达到多少个格子？ 思路：递归模拟机器人走路，用个列表记录机器人走过的路就行 Code 123456789101112131415161718class Solution_1: def movingCount(self, threshold, rows, cols): # write code here def Sum(x): res = 0 while x != 0: res += x % 10 x = x//10 return res def search(i,j,rows,cols,threshold,List=[]): if threshold&lt;0: return 0 if i&gt;=cols or j&gt;=rows or Sum(i)+Sum(j)&gt;threshold or [i,j] in List: return 0 else: List.append([i,j]) return 1+search(i+1,j,rows,cols,threshold,List)+search(i,j+1,rows,cols,threshold,List) return search(0,0,rows,cols,threshold) 二叉树的下一个节点Q:给定一个二叉树和其中的一个结点，请找出中序遍历顺序的下一个结点并且返回。注意，树中的结点不仅包含左右子结点，同时包含指向父结点的指针。 思路：有right就沿着right的left一直找下去找到尽头没有right就沿着父节点一直向上找，直到导找到该节点是父节点的left Code 123456789101112131415class Solution: def GetNext(self, pNode): # write code here if not pNode: return pNode p = pNode if p.right: post = p.right while post.left: post = post.left return post while p.next: if p.next.left == p: return p.next p = p.next 二叉树的深度Q:输入一棵二叉树，求该树的深度。从根结点到叶结点依次经过的结点（含根、叶结点）形成树的一条路径，最长路径的长度为树的深度。 思路1：用self.temp记录当前深度，self.Max记录最大深度。先序self.temp+1，后序self.temp-1注意：不要思维定式先中后序只用一个 Code 123456789101112131415class Solution: def __init__(self): self.temp = 0 self.Max = 0 def TreeDepth(self, pRoot): # write code here if not pRoot: return 0 self.temp += 1 if self.temp &gt; self.Max: self.Max = self.temp self.TreeDepth(pRoot.left) self.TreeDepth(pRoot.right) self.temp -= 1 return self.Max **思路2：更棒的方法 通过 左子树或右子树最大深度+1为当前子树深度 进行递归 Code 12345678class Solution: def TreeDepth(self, pRoot): # write code here if not pRoot: return 0 left = self.TreeDepth(pRoot.left) right = self.TreeDepth(pRoot.right) return max(left, right)+1 平衡二叉树Q:输入一棵二叉树，判断该二叉树是否是平衡二叉树。 思路1：写一个递归函数判断深度，注意对max(left,right)的运用，即左子树或右子树最大深度+1为当前子树的深度（写出递归的关键所在）然后从根节点开始递归判断每个节点 缺点：根节点开始递归判断每个节点缺点：重复遍历节点，时间复杂度高 Code 1234567891011121314class Solution: def IsBalanced_Solution(self, pRoot): # write code here if not pRoot: return True if abs( self.TreeDepth(pRoot.left) - self.TreeDepth(pRoot.right) ) &gt; 1: return False return self.IsBalanced_Solution(pRoot.left) and self.IsBalanced_Solution(pRoot.right) def TreeDepth(self, pRoot): if not pRoot: return 0 left = self.TreeDepth(pRoot.left) right = self.TreeDepth(pRoot.right) return max(left,right) + 1 思路2：同样写一个递归函数判断深度，但是一旦出现不满足返回值便为 -1 ，同时在后序中 left &lt; 0 or right &lt; 0 的运用保证了一旦出现非平衡子树，-1就一直会传递到最后，最后只需在主函数中判断深度是否 &gt;=0 。 对 -1 的传递真是太赞，好好体会 Code 123456789101112class Solution: def IsBalanced_Solution(self, pRoot): # write code here return self.TreeDepth(pRoot) &gt;= 0 def TreeDepth(self, pRoot): if not pRoot: return 0 left = self.TreeDepth(pRoot.left) right = self.TreeDepth(pRoot.right) if (left &lt; 0 or right &lt; 0 or abs(left - right) &gt; 1): return -1 return max(left, right) + 1 对称的二叉树Q:请实现一个函数，用来判断一颗二叉树是不是对称的。注意，如果一个二叉树同此二叉树的镜像是同样的，定义其为对称的。 思路：写递归函数（输入为两个节点）递归比较，递归方式为haha(left.left,right.right) and haha(left.right,right.left) Code 123456789101112class Solution: def isSymmetrical(self, pRoot): # write code here if not pRoot: return True return self.haha(pRoot.left,pRoot.right) def haha(self,left,right): if not left and not right: return True if not left or not right or left.val != right.val : return False return self.haha(left.left,right.right) and self.haha(left.right,right.left) 把二叉树打印成多行Q:从上到下按层打印二叉树，同一层结点从左至右输出。每一层输出一行。 思路：据层序遍历修改。将val按照嵌套列表方式存储（即每个元素list为一层的val）。while 每次循环处理一层，NextLayer为贮存下层节点的临时list，遍历处理完本层节点后通过Nodeque = NextLayer一下子将本层节点更换为下层节点来推进循环。 Code 1234567891011121314151617181920class Solution: # 返回二维列表[[1,2],[4,5]] def Print(self, pRoot): # write code here if not pRoot: return [] NodeList = [pRoot] res = [] while NodeList: NextLayer = [] ValList = [] for node in NodeList: ValList.append(node.val) if node.left: NextLayer.append(node.left) if node.right: NextLayer.append(node.right) res.append(ValList) NodeList = NextLayer return res 按之字形打印二叉树Q:地上有一个m行和n列的方格。一个机器人从坐标0,0的格子开始移动，每一次只能向左，右，上，下四个方向移动一格，但是不能进入行坐标和列坐标的数位之和大于k的格子。 例如，当k为18时，机器人能够进入方格（35,37），因为3+5+3+7 = 18。但是，它不能进入方格（35,38），因为3+5+3+8 = 19。请问该机器人能够达到多少个格子？ 思路：据层序遍历修改。将val按照嵌套列表方式存储（即每个元素list为一层的val）。while 每次循环处理一层，NextLayer为贮存下层节点的临时list，遍历处理完本层节点后通过Nodeque = NextLayer一下子将本层节点更换为下层节点来推进循环。最后按照奇偶顺序修改val列表即可。 注意：将节点加入NextLayer时要判断存在与否，否则空类型也会被添加 Code 12345678910111213141516171819202122232425262728293031class Solution: def Print(self, pRoot): # write code here if not pRoot: return [] # Nodeque为循环遍历的node list Nodeque = [pRoot] # res为嵌套list,每个元素list为每层的val res = [] # 通过Nodeque = NextLayer一下子将本层节点更换为下层节点来推进循环 # 即每一圈while为每层的处理 while Nodeque: # NextLayer为本层节点的所有孩子节点 NextLayer = [] # ValList存取本层所有节点的val ValList = [] for node in Nodeque: ValList.append(node.val) if node.left: NextLayer.append(node.left) if node.right: NextLayer.append(node.right) res.append(ValList) Nodeque = NextLayer transres = [] for i,v in enumerate(res): if i % 2: transres.append(v[::-1]) else: transres.append(v) return transres 两个链表的第一个公共节点Q:输入两个链表，找出它们的第一个公共结点。这里的是指两个链表在某个节点之后会汇入同一个链表。 思路：最后两个链表汇入一个链表，即最后公共长度是一样的。因此可以先遍历两个链表，得出长度差在较长链表上提前多走长度差的步数，再一一比较 Code 123456789101112131415161718192021222324252627282930class Solution: def FindFirstCommonNode(self, pHead1, pHead2): # write code here if not pHead1 or not pHead2: return None p1 = pHead1 p2 = pHead2 length1 = 0 length2 = 0 while p1: length1 += 1 p1 = p1.next while p2: length2 += 1 p2 = p2.next p1 = pHead1 p2 = pHead2 differ = abs(length1 - length2) if length1 &gt; length2: for i in range(differ): p1 = p1.next else: for i in range(differ): p2 = p2.next while p1 and p2: if p1.val == p2.val: return p1 p1 = p1.next p2 = p2.next return None 数组中出现次数超过一半的元素Q:数组中有一个数字出现的次数超过数组长度的一半，请找出这个数字。例如输入一个长度为9的数组{1,2,3,2,2,2,5,4,2}。由于数字2在数组中出现了5次，超过数组长度的一半，因此输出2。如果不存在则输出0。 思路1：用哈希记录每个元素出现的次数 Code 1234567891011121314class Solution27: def MoreThanHalfNum_Solution(self, numbers): # write code here if len(numbers) == 1: return numbers[0] dic = &#123;&#125; for i in range(len(numbers)): if numbers[i] not in dic: dic[numbers[i]] = 1 else: dic[numbers[i]] += 1 if dic[numbers[i]]&gt;len(numbers)/2: return numbers[i] return 0 思路2：如果有个元素出现超过一半，那么这个元素比其他元素都多（废话）。据此，count记录所存储元素的计数，temp记录所存取元素。count == 0时，temp变为当前元素，count置1；count != 0时， 当前元素与temp相等，count+1否则-1;结束遍历一遍temp元素，看看是否次数超过一半。 Code 123456789101112131415161718192021222324class Solution: def MoreThanHalfNum_Solution(self, numbers): # write code here count = 0 temp = -99 for i in numbers: if count == 0: temp = i count += 1 else: if i != temp: count -= 1 else: count += 1 if count == 0: return 0 kan = 0 for i in numbers: if i == temp: kan += 1 if kan &gt; len(numbers) // 2: return temp else: return 0 丑数Q:把只包含因子2、3和5的数称作丑数（Ugly Number）。例如6、8都是丑数，但14不是，因为它包含因子7。 习惯上我们把1当做是第一个丑数。求按从小到大的顺序的第N个丑数。 思路：考虑o(N)的用丑数生成丑数，不再遍历多余的非丑数。建立uglynums列表按顺序保存丑数。除了1以外，丑数肯定是排在之前的丑数乘以2、3、5的结果。问题在于怎样确保列表里面的丑数是排好顺序的。假设最大丑数为M，计算下一个丑数时小于M的肯定已经在列表当中。对于乘以2而言，肯定存在某一个丑数，排在它之前的每个丑数乘以2都小于M，排在它之后的每个丑数乘以2都远大于M，我们只需要记录下这个丑数的位置m2，对于乘以3和5，同样存下m3、m5. 12345注意： 1.更新标记只需要 +1 即可，+1之后再用到肯定大于M 2.此代码精彩的地方在于省略了对新丑数是否=M的判断，见下一条 3.有可能有几个数同时得到要添加的数值，如6（2*3，3*2）则m2,m3需要同时更新，这里的三个并列的if相当于省略了对更新的数是否=M的判断 Code 123456789101112131415161718class Solution: def GetUglyNumber_Solution(self, index): # write code here uglynums = [1] m2 = m3 = m5 =0 if index &lt; 1: return 0 if index == 1: return 1 for i in range(1,index+1): uglynums.append(min(uglynums[m2]*2, uglynums[m3]*3, uglynums[m5]*5)) if uglynums[i] == uglynums[m2]*2: m2 += 1 if uglynums[i] == uglynums[m3]*3: m3 += 1 if uglynums[i] == uglynums[m5]*5: m5 += 1 return uglynums[index-1] 数字在排序数组中出现的次数Q:统计一个数字在排序数组中出现的次数。 思路：二分查找找到一个target，返回index，然后往两边扩展 Code 12345678910111213141516171819202122232425262728293031class Solution: def Getindex(self, data, left, right, k): if left &gt; right: return -1 mid = (left + right) // 2 if data[mid] == k: return mid if data[mid] &lt; k: return self.Getindex(data, mid+1, right,k) else: return self.Getindex(data,left,mid-1,k) def GetNumberOfK(self, data, k): # write code here if not data: return 0 left = 0 right = len(data) - 1 flag = self.Getindex(data, left, right, k) if flag &lt; 0: return 0 else: l = r = flag while l &gt;= 1 and data[l-1] == k: l -= 1 if data[0] == k: l =0 while r &lt;= len(data) - 2 and data[r+1] == k: r += 1 if data[right] == k: r = right return r - l + 1 数组中的逆序对Q：在数组中的两个数字，如果前面一个数字大于后面的数字，则这两个数字组成一个逆序对。输入一个数组,求出这个数组中的逆序对的总数P。 思路：根据归并排序改编，每次合并两个数组排序时，查找两个数组中的逆序对，因为单个数组已经排好序，所以只需考虑两个数组之间的逆序对。具体方法是，见代码注释 12345678910111213141516171819202122232425262728class Solution34: def __init__(self): self.count = 0 def InversePairs(self, data): # write code here self.guibing(data) return self.count def guibing(self,data): if len(data) == 1: return data mid = len(data) // 2 left = self.guibing(data[:mid]) right = self.guibing(data[mid:]) # left_len 用于left、right合并最后right剩余时候计算剩余right的逆序对 left_len = len(left) res = [] while left and right: # 比较left、right最后一位，将大的从头插入res # 如果left[-1]大，此时right的长度就是left[-1]对应的逆序对数 if left[-1] &gt; right[-1]: self.count += len(right) res.insert(0, left.pop(-1)) else: res.insert(0, right.pop(-1)) if left: return left + res if right: return right + res]]></content>
      <categories>
        <category>编程</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>代码</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LeetCode]]></title>
    <url>%2F2017%2F10%2F26%2FLeetCode%2F</url>
    <content type="text"><![CDATA[LeetCode1.Two SumQ:Given an array of integers, return indices of the two numbers such that they add up to a specific target. You may assume that each input would have exactly one solution, and you may not use the same element twice. Example: 123Given nums = [2, 7, 11, 15], target = 9,Because nums[0] + nums[1] = 2 + 7 = 9,return [0, 1]. 思路：建立字典，key为值，val为值所对应位置 Code 12345678910111213class Solution: def twoSum(self, nums, target): """ :type nums: List[int] :type target: int :rtype: List[int] """ tag = &#123;&#125; for i in range(len(nums)): if nums[i] in tag: return[tag[nums[i]],i] else: tag[target-nums[i]]=i 2.Add Two NumbersQ:You are given two non-empty linked lists representing two non-negative integers. The digits are stored in reverse order and each of their nodes contain a single digit. Add the two numbers and return it as a linked list. You may assume the two numbers do not contain any leading zero, except the number 0 itself. Example: 123Input: (2 -&gt; 4 -&gt; 3) + (5 -&gt; 6 -&gt; 4)Output: 7 -&gt; 0 -&gt; 8Explanation: 342 + 465 = 807. 思路：很简单 Code 123456789101112131415161718192021222324252627class ListNode(object): def __init__(self, x): self.val = x self.next = Noneclass Solution(object): def addTwoNumbers(self, l1, l2): """ :type l1: ListNode :type l2: ListNode :rtype: ListNode """ takeover = 0 root = n = ListNode(0) while l1 or l2 or takeover: if l1: takeover += l1.val l1 = l1.next if l2: takeover += l2.val l2 = l2.next takeover, val = divmod(takeover, 10) # 以下两行可以写成 # n.next = n = ListNode(val) n.next = ListNode(val) n = n.next return root.next 3.Longest Substring Without Repeating CharactersQ:Given a string, find the length of the longest substring without repeating characters. Example: 12345Given "abcabcbb", the answer is "abc", which the length is 3.Given "bbbbb", the answer is "b", with the length of 1.Given "pwwkew", the answer is "wke", with the length of 3. Note that the answer must be a substring, "pwke" is a subsequence and not a substring. 思路：建立字典key为值，val为该值最后一次出现的位置，用指针pre指向每次寻找的子串的开头位置 注意：自己思路是对的，但是错在没考虑嵌套重复如 abcddoua 遍历到第二个d时候重复pre指向第二个d 而遍历到第二个a时pre指向第一个a后的b，显然错的（即pre应该始终在字典回溯值之前，pre只能向后移动） Code 1234567891011def lengthOfLongestSubstringonglen(s): usedchar = &#123;&#125; maxlen = 0 pre = 0 for i in range(len(s)): if s[i] in usedchar and pre &lt;= usedchar[s[i]]: pre = usedchar[s[i]] + 1 else: maxlen = max(maxlen, i-pre+1) usedchar[s[i]] = i return maxlen 5. Longest Palindromic SubstringQ:Given a string s, find the longest palindromic substring in s. You may assume that the maximum length of s is 1000. Example: 12345Input: "babad"Output: "bab"Note: "aba" is also a valid answer. 思路： ①for遍历选中心元素，找齐所有中心元素（第一个while 向后 找出所有与所选中心元素重复的元素） ② l r 指示回文边界，向两遍扩展 注意： ①l r 始终指向有效边界（闭区间，考虑边界值仍然是回文区域） ②注意边界情况判断，如第一个while循环判定条件有两个，应该把边界判断放在前面，要不会出现index越界 Code 1234567891011121314151617def longestPalindrome(s): if len(s) &lt;= 2: return s maxlen = real_l = real_r = 0 for i in range(len(s) - 1): l, r, pos = i, i, i+1 while ( pos &lt;= len(s) - 1) and s[r] == s[pos]: r = pos pos += 1 while (l &gt;= 1 and r &lt; (len(s) - 1)) and s[l - 1] == s[r + 1]: l -= 1 r += 1 if r - l + 1 &gt; maxlen: maxlen = r - l + 1 real_l = l real_r = r return s[real_l:real_r+1] 7. Reverse IntegerQ:Given a 32-bit signed integer, reverse digits of an integer. Example: 123456Input: 123Output: 321Input: -123Output: -321Input: 120Output: 21 思路： ①flag表示符号判断正数1负数-1 ②将int转为字符串形式，[::-1]表示字符串反转 ③bit_length表示位数的判断 注意： 注意对大数的判断 Code 1234def reverse(x): flag = (x&gt;0)-(x&lt;0) val = flag*int(str(abs(x))[::-1]) return val*(val.bit_length()&lt;32) 8. String to Integer (atoi)Q:Implement atoi to convert a string to an integer. Example: 12 思路：此题太乱没做，总结答案 str.strip（）用于移除首位指定字符串 str.isdigit()判断字符是否是数值 ord()将字符转换为ASCII码 chr()将ASCII码转为字符如ord(‘a’) -&gt; 97 chr(97) -&gt; ‘a’4. 字符串跟数值问题注意overflow问题，这里是有符号函数所以max=231-1 min=-231 注意： 注意对大数的判断 Code 1234567891011121314151617def myAtoi(self, s): """ :type str: str :rtype: int """ ###better to do strip before sanity check (although 8ms slower): if len(s) == 0: return 0 ls = list(s.strip()) sign = -1 if ls[0] == '-' else 1 if ls[0] in ['-', '+']: del ls[0] ret, i = 0, 0 while i &lt; len(ls) and ls[i].isdigit(): ret = ret * 10 + ord(ls[i]) - ord('0') i += 1 return max(-2 ** 31, min(sign * ret, 2 ** 31 - 1)) 9. Palindrome NumberQ:Determine whether an integer is a palindrome. Do this without extra space. Example: 12 思路： 1.负数不是回文数、非零且末位为零不是回文数 2.用数字一半部分来比较避免了翻转数字overflow问题（赞） 注意： 注意对大数的处理（用数字一半部分作比较） Code 123456789101112def isPalindrome(self, x): """ :type x: int :rtype: bool """ if x &lt; 0 or (x % 10 == 0 and x != 0): return False res = 0 while x &gt; res: res = res * 10 + x % 10 x = x // 10 return (x == res or x == res // 10) 11.Container With Most WaterQ:Given n non-negative integers a1, a2, …, an, where each represents a point at coordinate (i, ai). n vertical lines are drawn such that the two endpoints of line i is at (i, ai) and (i, 0). Find two lines, which together with x-axis forms a container, such that the container contains the most water. Example: 12 思路：分析此题可以得到一个解题基础：最终得到的两块挡板左挡板L和右挡板R，L左边的所有挡板一定短于L，R右边的所有挡板一定短于R。据此，可以从轴两遍开始向内遍历，每次记录最值，并且下次从较短的挡板继续向内遍历。 注意： 注意对python 中用while True: if XX: break 来代替do while Code 1234567891011121314151617181920212223242526272829def maxArea( height): """ :type height: List[int] :rtype: int """ l = 0 r = len(height) - 1 l_h = height[l] r_h = height[r] val = maxval = min(l_h, r_h) * (r - l) while (l &lt; r): if l_h &lt;= r_h: while True: l += 1 if height[l] &gt; l_h or l &gt;= r: if r &gt; l: l_h = height[l] break elif l_h &gt; r_h: while True: r -= 1 if height[r] &gt; r_h or l &gt;= r: if r &gt; l: r_h = height[r] break val = min(l_h, r_h) * (r - l) if val &gt; maxval: maxval = val return maxval 14.Longest Common PrefixQ:Write a function to find the longest common prefix string amongst an array of strings. Example: 1找到字符串们的最长前缀 思路：关于zip()以及zip(* )的用法 前者相当于压缩到一个list中，后者相当于分开 12345678910111213141516a=[1,2,3]b=[4,5,6,7]c=[8,9,10,11,12]zz=zip(a,b,c)print(zz)x,y,z=zip(*zz)print(x)print(y)print(z)输出：[(1, 4, 8), (2, 5, 9), (3, 6, 10)](1, 2, 3)(4, 5, 6)(8, 9, 10) Code 1234567891011def longestCommonPrefix(self, strs): """ :type strs: List[str] :rtype: str """ if len(strs) == 0: return '' for i, group in enumerate(zip(*strs)): if len(set(group)) != 1: return strs[0][:i] return min(strs) 15.3 SumQ:Given an array S of n integers, are there elements a, b, c in S such that a + b + c = 0? Find all unique triplets in the array which gives the sum of zero. Note: The solution set must not contain duplicate triplets. Example: 1234567For example, given array S = [-1, 0, 1, 2, -1, -4],A solution set is:[ [-1, 0, 1], [-1, -1, 2]] 思路：开头 排序 为了跳过重复元素（用dup记录上一元素）遍历一次用twosum函数返回（twosum函数也要去重） Code 12345678910111213141516171819202122232425class Solution: def threeSum(self, nums): """ :type nums: List[int] :rtype: List[List[int]] """ nums = sorted(nums) result = [] dup = -999 for i in range(len(nums)-1): if nums[i] != dup: result.extend(self.twosum(nums[i],nums[i+1:],nums[i]*-1)) dup = nums[i] return result def twosum(self,p,arr,val): box = set() threesum = [] for i in range(len(arr)): if (val-arr[i] in box): temp = sorted([p,arr[i],val-arr[i]]) if temp not in threesum: threesum.append(temp) else: box.add(arr[i]) return threesum 16.3 Sum ClosestQ:Given an array S of n integers, find three integers in S such that the sum is closest to a given number, target. Return the sum of the three integers. You may assume that each input would have exactly one solution. Example: 123For example, given array S = &#123;-1 2 1 -4&#125;, and target = 1. The sum that is closest to the target is 2. (-1 + 2 + 1 = 2). 思路：排序后根据大小查找 Code 12345678910111213141516171819202122def threeSumClosest(self, nums, target): """ :type nums: List[int] :type target: int :rtype: int """ nums.sort() close = nums[0]+nums[1]+nums[2] for i in range(len(nums)-1): j = i+1 k = len(nums)-1 while j&lt;k: sum = nums[i]+nums[j]+nums[k] if sum == target: return sum if abs(target-sum) &lt; abs(target-close): close = sum if sum &lt; target: j += 1 elif sum &gt; target: k -= 1 return close 17.Letter Combinations of a Phone NumbertQ:Given a digit string, return all possible letter combinations that the number could represent. A mapping of digit to letters (just like on the telephone buttons) is given below. Example: 12Input:Digit string "23"Output: ["ad", "ae", "af", "bd", "be", "bf", "cd", "ce", "cf"]. 思路：递归思想 字符串拼接用 + Code 1234567891011121314def letterCombinations(self, digits): """ :type digits: str :rtype: List[str] """ mapping = &#123;'0': ' ', '1': '', '2': 'abc', '3': 'def', '4': 'ghi', '5': 'jkl' , '6': 'mno', '7': 'pqrs', '8': 'tuv', '9': 'wxyz'&#125; if len(digits) == 0: return [] if len(digits) == 1: return list(mapping[digits[0]]) pre_comb = self.letterCombinations(digits[:-1]) last = mapping[digits[-1]] return [s + v for s in pre_comb for v in last] 18.4 SumQ:Given an array S of n integers, are there elements a, b, c, and d in S such that a + b + c + d = target? Find all unique quadruplets in the array which gives the sum of target. Note: The solution set must not contain duplicate quadruplets. Example: 12345678For example, given array S = [1, 0, -1, 0, -2, 2], and target = 0.A solution set is:[ [-1, 0, 0, 1], [-2, -1, 1, 2], [-2, 0, 0, 2]] 思路：列表之间的拼接用 ‘+’先两层嵌套for循环将所有两两组合之和求出来，再用twosum思想 去重 保证每个元素只添加一次(通过变量保存下标而不是值) Code 1234567891011121314151617181920212223def fourSum(self, nums, target): """ :type nums: List[int] :type target: int :rtype: List[List[int]] """ dic = &#123;&#125; result = [] for i in range(len(nums) - 1): for j in range(i + 1, len(nums)): twosum = nums[i] + nums[j] if target - twosum in dic: for two in range(len(dic[target - twosum])): thisfour = sorted([nums[i], nums[j]] + [nums[dic[target - twosum][two][0]], nums[dic[target - twosum][two][1]]]) # and前条件用于去重；and后条件用于去除元素是否重复利用 if thisfour not in result and len(set([i, j]) | set(dic[target - twosum][two])) == 4: result.append(thisfour) if twosum not in dic: dic[twosum] = [[i, j]] else: dic[twosum].append([i, j]) return result 19.Remove Nth Node From End of ListQ:Given a linked list, remove the nth node from the end of list and return its head. Note:Given n will always be valid.Try to do this in one pass. Example: 123Given linked list: 1-&gt;2-&gt;3-&gt;4-&gt;5, and n = 2.After removing the second node from the end, the linked list becomes 1-&gt;2-&gt;3-&gt;5. 思路：构造一个长度为n的滑窗，滑到最后注意判断如果删除了第一个元素（此时post.next = null）应返回head.next Code 123456789101112131415161718def removeNthFromEnd(self, head, n): """ :type head: ListNode :type n: int :rtype: ListNode """ pre = head post = head for i in range(n): post = post.next # 注意这一步，如果删除的是head，直接返回head.next if not post: return head.next while post.next: pre = pre.next post = post.next pre.next = pre.next.next return head 20. Valid ParenthesesQ:Given a string containing just the characters &#39;(&#39;, &#39;)&#39;, &#39;{&#39;, &#39;}&#39;, &#39;[&#39; and &#39;]&#39;, determine if the input string is valid. The brackets must close in the correct order, &quot;()&quot; and &quot;()[]{}&quot; are all valid but &quot;(]&quot; and &quot;([)]&quot; are not.. Example: 12 思路：not valid 只有三种情况，找准这三种情况的特点注意一手对ord()的应用 Code 1234567891011121314151617181920def isValid(self, s): """ :type s: str :rtype: bool """ stack = [] for i in range(len(s)): if s[i] in ['(','&#123;','[']: stack = stack + [s[i]] else: if len(stack)==0: # 右括号比左括号多的情况 return False if ord(s[i])-ord(stack[-1])!= 1 and ord(s[i])-ord(stack[-1])!= 2: #左右括号不对称的情况 return False else: stack = stack[:-1] if len(stack) == 0: return True else: # 左括号比右括号多的情况 return False 21. Merge Two Sorted ListsQ:Merge two sorted linked lists and return it as a new list. The new list should be made by splicing together the nodes of the first two lists. Example: 12Input: 1-&gt;2-&gt;4, 1-&gt;3-&gt;4Output: 1-&gt;1-&gt;2-&gt;3-&gt;4-&gt;4 思路： 1.正常思路，用and终止while，l1 或 l2 的尾巴可以一并加上 2.递归思想找准结束条件，链表一个一个拼接 Code 123456789101112131415161718192021222324252627282930313233# 第一种做法def mergeTwoLists( l1, l2): """ :type l1: ListNode :type l2: ListNode :rtype: ListNode """ head = p = ListNode(0) while l1 and l2: if l1.val &lt;= l2.val: p.next = l1 l1 = l1.next else: p.next = l2 l2 = l2.next p = p.next p.next = l1 or l2 #加上小尾巴 return head.next# 递归做法def mergeTwoLists(self, l1, l2): """ :type l1: ListNode :type l2: ListNode :rtype: ListNode """ if not l1 or not l2: return l1 or l2 if l1.val &lt; l2.val: l1.next = self.mergeTwoLists(l1.next, l2) return l1 else: l2.next = self.mergeTwoLists(l1, l2.next) return l2 22.Generate ParenthesesQ:Given n pairs of parentheses, write a function to generate all combinations of well-formed parentheses. For example, given n = 3, a solution set is: Example: 1234567[ "((()))", "(()())", "(())()", "()(())", "()()()"] 思路： 不带返回值的递归（用于添加或者修改），递归生成每个str Left right 分别为左右括号的计数器 Init_str为目前生成的字符串 Code 123456789101112131415161718192021class Solution22: def generateParenthesis(self, n): """ :type n: int :rtype: List[str] """ if not n: return [] left, right, init_str = n, n, '' result = [] self.generate(left,right,init_str,result) return result def generate(self,left,right,init_str,result): if not left and not right: result.append(init_str) return if left: self.generate(left-1,right,init_str+'(',result) #left &lt; right 为添加右括号条件 if right and left &lt; right: self.generate(left,right-1,init_str+')',result) 24.wap Nodes in PairsQ:Given a linked list, swap every two adjacent nodes and return its head. For example,Given 1-&gt;2-&gt;3-&gt;4, you should return the list as 2-&gt;1-&gt;4-&gt;3. Your algorithm should use only constant space. You may not modify the values in the list, only nodes itself can be changed. Example: 12input:1-&gt;2-&gt;3-&gt;4output:2-&gt;1-&gt;4-&gt;3. 思路：基本操作，但是遇到return head报错，return init.next就正确，没找到原因 Code 123456789101112131415161718192021def swapPairs(self, head): """ :type head: ListNode :rtype: ListNode """ init = ListNode(0) pre = init pre.next = head while pre.next and pre.next.next: p = pre.next post = p.next p.next = post.next post.next = p pre.next = post if not p.next: break else: pre = p p = p.next post = p.next return init.next 24.wap Nodes in PairsQ:Given a linked list, swap every two adjacent nodes and return its head. For example,Given 1-&gt;2-&gt;3-&gt;4, you should return the list as 2-&gt;1-&gt;4-&gt;3. Your algorithm should use only constant space. You may not modify the values in the list, only nodes itself can be changed. Example: 12input:1-&gt;2-&gt;3-&gt;4output:2-&gt;1-&gt;4-&gt;3. 思路：基本操作，但是遇到return head报错，return init.next就正确，没找到原因 Code 123456789101112131415161718192021def swapPairs(self, head): """ :type head: ListNode :rtype: ListNode """ init = ListNode(0) pre = init pre.next = head while pre.next and pre.next.next: p = pre.next post = p.next p.next = post.next post.next = p pre.next = post if not p.next: break else: pre = p p = p.next post = p.next return init.next 26.Remove Duplicates from Sorted ArrayQ:Given a sorted array, remove the duplicates in-place such that each element appear only once and return the new length. Do not allocate extra space for another array, you must do this by modifying the input array in-place with O(1) extra memory. Example: 1234Given nums = [1,1,2],Your function should return length = 2, with the first two elements of nums being 1 and 2 respectively.It doesn't matter what you leave beyond the new length. 思路：简单题，在数组内删除重复元素，两个指针write、read,修改时候 A[write + 1] = A[read] Code 12345678910111213141516171819202122class Solution26(object): def removeDuplicates(self, A): """ :type nums: List[int] :rtype: int """ if not A: return 0 if len(A) == 1: return 1 write = 0 read = 1 while read &lt; len(A): if A[write] != A[read]: # 修改重复元素的方法 A[write + 1] = A[read] write += 1 read += 1 else: read += 1 A = A[:write + 1] return len(A) 27.Remove ElementQ:Given an array and a value, remove all instances of that value in-place and return the new length. Do not allocate extra space for another array, you must do this by modifying the input array in-place with O(1) extra memory. The order of elements can be changed. It doesn’t matter what you leave beyond the new length. Example: 123Given nums = [3,2,2,3], val = 3,Your function should return length = 2, with the first two elements of nums being 2. 思路：简单的双指针 Code 1234567891011121314151617class Solution27: def removeElement(self, nums, val): """ :type nums: List[int] :type val: int :rtype: int """ if not nums: return 0 read = write =0 while read &lt; len(nums): if nums[read] != val: nums[write] = nums[read] write += 1 read += 1 nums = nums[:write] return len(nums) 29.Divide Two IntegersQ:Divide two integers without using multiplication, division and mod operator. If it is overflow, return MAX_INT. Example: 12 思路：不会位运算，看了discussion此题两个while循环，第一个为结束条件，第二个循环每次减去除数的1、2、4、8……倍，结束第二个循环继续从一倍开始。减完回到第一个循环的判定条件判断是否需要再进行第二个while，相当于指数高效的记录倍数 Code 1234567891011121314151617181920class Solution29: def divide(self, dividend, divisor): """ :type dividend: int :type divisor: int :rtype: int """ positive = (dividend &lt; 0) is (divisor &lt; 0) dividend, divisor = abs(dividend), abs(divisor) res = 0 while dividend &gt;= divisor: temp, i = divisor, 1 while dividend &gt;= temp: dividend -= temp res += i temp &lt;&lt;= 1 i &lt;&lt;= 1 if not positive: res = -res return min(max(-2 ** 31,res), 2**31-1) 31.Next PermutationQ:Implement next permutation, which rearranges numbers into the lexicographically next greater permutation of numbers. If such arrangement is not possible, it must rearrange it as the lowest possible order (ie, sorted in ascending order). The replacement must be in-place, do not allocate extra memory. Here are some examples. Inputs are in the left-hand column and its corresponding outputs are in the right-hand column. 从这几个数组合中找到下一个比他大的数 Example: 1231,2,3 → 1,3,23,2,1 → 1,2,31,1,5 → 1,5,1 思路：较简单，尽可能更改最后几位，并且从后往前如果一直是递增则就是这几个数的最大排列如4321，从后找到第一个非递增的数字p（如1245763中的5），然后将此数字与之后数字中 大于它且最接近于它 的数字（6）交换，交换后将该位置之后的数字升序排列（1246357）该题要求在原数组中修改，因此我把之后按升序拍了的数组建立一个临时数组temp_list，用for循环写入原数组 list.sort() 和 sorted(list)的区别 12345678a = [3,2,1]list.sort()就地修改无返回值 sorted(list)返回新列表，对所有可迭代对象均有效print(a.sort()) -&gt; Noneprint(sorted(a)) -&gt; [1,2,3]a.sort() print(a) -&gt; [1,2,3]#对于切片列表：a[1:].sort() print(a) -&gt;[3,2,1] 无效print(sorted(a[1:])) -&gt;[1,2] 注意： ①找与p交换的数字时从后往前找可以避免index越界问题 ②考虑有重复数字问题 ③while遍历时候考虑左右边界问题（如while p and 条件 -&gt;此为不超过左边界） Code 12345678910111213141516171819202122232425262728class Solution31: def nextPermutation(self, nums): """ :type nums: List[int] :rtype: void Do not return anything, modify nums in-place instead. """ if not nums: return None p = len(nums) - 1 # while p用以阻止左边界越界 while p and nums[p-1] &gt;= nums[p]: p -= 1 if p == 0: nums.sort() return None else: p = p - 1 post = len(nums) - 1 # while post用以阻止左边界越界，同时从后往前找防止右边界越界 while post and nums[post]&lt;=nums[p]: post -= 1 temp = nums[p] nums[p] = nums[post] nums[post] = temp temp_list = sorted(nums[p+1:]) for i in range(p+1,len(nums)): nums[i] = temp_list[i-p-1] return None 33.Search in Rotated Sorted ArrayQ:Suppose an array sorted in ascending order is rotated at some pivot unknown to you beforehand. (i.e., 0 1 2 4 5 6 7 might become 4 5 6 7 0 1 2). You are given a target value to search. If found in the array return its index, otherwise return -1. You may assume no duplicate exists in the array. Example: 12 思路：在排序或类排序数组中查找首先想到二分法此题因为不需要考虑重复元素，较为简单，列出所有情况来即可，我的代码比较繁琐，边界值判定条件应该可以统一一下，懒得改了因为最后的6种判定条件互斥，所以用 抑或^ 或者 or 都行分情况讨论如图（类似高中数学题） Code 12345678910111213141516171819202122232425class Solution33: def search(self, nums, target): """ :type nums: List[int] :type target: int :rtype: int """ if len(nums) == 1 and nums[0] == target: return 0 left = 0 right = len(nums)-1 while left &lt; right: mid = (left +right) // 2 if nums[left] == target: return left if nums[right] == target: return right if nums[mid] == target: return mid if (nums[mid]&gt;target and target &gt; nums[right])^(nums[right]&gt;nums[mid] and nums[mid]&gt;target )^(target&gt;nums[right] and nums[right]&gt;nums[mid]): right = mid - 1 else: left = mid + 1 return -1 34.Search for a RangeQ:Given an array of integers sorted in ascending order, find the starting and ending position of a given target value. Your algorithm’s runtime complexity must be in the order of O(log n). If the target is not found in the array, return [-1, -1]. Example: 12Given [5, 7, 7, 8, 8, 10] and target value 8,return [3, 4]. 思路：很简单，因为是排序数组，所以先用二分法查找target,在从这个位置往两边扩展 注意： ①类似while pre&gt;0 and nums[pre] == target: 中判断while结束后的pre是否指向target(有可能因为边界条件跳出循环，并且边界仍满足target)来判断pre是否 -1 ②第一个while l&lt;=r: 中‘=’存在不构成死循环的原因如果l=r且指向target接下来会return，如果l=r不指向target之后对于mid与target判定仍会跳出循环 Code 12345678910111213141516171819202122232425262728293031class Solution34: def searchRange(self, nums, target): """ :type nums: List[int] :type target: int :rtype: List[int] """ if not nums or (len(nums)==1 and nums[0] != target): return [-1,-1] l = 0 r = len(nums) - 1 if l == r and nums[0]==target: return [0,0] while l &lt;= r: mid = (l+r) // 2 if nums[mid] == target: pre = post = mid while pre&gt;0 and nums[pre] == target: pre -= 1 while post&lt;len(nums)-1 and nums[post] == target: post += 1 if nums[pre] != target: pre += 1 if nums[post] != target: post -= 1 return [pre, post] elif nums[mid] &gt; target: r = mid - 1 else: l = mid + 1 return [-1, -1] 35.Search Insert PositionQ:Given a sorted array and a target value, return the index if the target is found. If not, return the index where it would be if it were inserted in order. You may assume no duplicates in the array. Example: 12345678Input: [1,3,5,6], 5Output: 2Input: [1,3,5,6], 2Output: 1Input: [1,3,5,6], 7Output: 4Input: [1,3,5,6], 0Output: 0 思路：没什么好说的，排序数列二分查找，注意边界值问题及while中的等号问题while 中 加’=’保证在无target的情况，mid在连续两位中的右边一位 注意：target&gt;max情况 Code 12345678910111213141516171819202122232425class Solution35: def searchInsert(self, nums, target): """ :type nums: List[int] :type target: int :rtype: int """ if not nums or (len(nums)==1 and nums[0]&gt;=target): return 0 if len(nums)==1 and nums[0]&lt;target: return 1 l = 0 r = len(nums)-1 while l &lt;= r: # 加‘=’保证在无target的情况，mid在连续两位中的右边一位 mid = (l + r) // 2 if nums[mid] == target: return mid elif nums[mid] &lt; target: l = mid + 1 else: r = mid - 1 if nums[mid]&gt;target: return mid else: # target&gt;max情况 return mid+1 **]]></content>
      <categories>
        <category>编程</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>代码</tag>
      </tags>
  </entry>
</search>
