<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[（转）从贝叶斯角度理解最小二乘法及L1、L2正则项]]></title>
    <url>%2F2017%2F12%2F16%2F%E2%80%9C%EF%BC%88%E8%BD%AC%EF%BC%89%E4%BB%8E%E8%B4%9D%E5%8F%B6%E6%96%AF%E8%A7%92%E5%BA%A6%E7%90%86%E8%A7%A3%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95%E5%8F%8AL1%E3%80%81L2%E6%AD%A3%E5%88%99%E9%A1%B9%E2%80%9D%2F</url>
    <content type="text"><![CDATA[文章转载自：知乎用户bsdelf，版权归原作者所有。 从概率论的角度： Least Square 的解析解可以用 Gaussian 分布以及最大似然估计求得 Ridge 回归可以用 Gaussian 分布和最大后验估计解释 LASSO 回归可以用 Laplace 分布和最大后验估计解释 首先假设线性回归模型具有如下形式： 其中，，误差。 当前已知，，怎样求呢？ 策略1. 假设，也就是说，那么用最大似然估计推导： 这不就是最小二乘么。 策略2. 假设，，那么用最大后验估计推导： 这不就是 Ridge 回归么？ 策略3. 假设，，同样用最大后验估计推导： 这不就是 LASSO 么？]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>贝叶斯</tag>
        <tag>正则项</tag>
        <tag>最小二乘法</tag>
      </tags>
  </entry>
</search>
